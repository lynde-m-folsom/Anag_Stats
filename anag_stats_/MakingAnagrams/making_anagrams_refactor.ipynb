{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cca7df1d",
   "metadata": {},
   "source": [
    "<h2> Generating the Stimuli file</h2>\n",
    "    \n",
    "This notebook is a modified version of prior work to generate the stimuli files used in anagram experiments.\n",
    "\n",
    "First we grab a list of words and then shuffle. From the shuffled set we then set up seperate groups (set groups) from there we use those paired shuffle and \"correct\" words to make the stimuli file used in the anagram experiments. \n",
    "\n",
    "This one will differ since I'll be including the sources and frequency data on use of the words in the set. Furthermore, I will be using this notebook to also make a json object of valid solutions for each shuffle. The stimuli file we use includes the word used for shuffling however, those shuffled strings can be solved to more than one real english word which is how we define \"valid\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ce3545",
   "metadata": {},
   "source": [
    "We can use dictionaries that have all the words of specific strings than use a function to give us some number of those words. \n",
    "\n",
    "#### The word bank we are using is from Word Net:\n",
    "    George A. Miller (1995). WordNet: A Lexical Database for English.\n",
    "    Communications of the ACM Vol. 38, No. 11: 39-41.\n",
    "    Christiane Fellbaum (1998, ed.) WordNet: An Electronic Lexical Database. Cambridge, MA: MIT Press.\n",
    "    WordNet: An Electronic Lexical Database\n",
    "\n",
    "#### The frequency information is from the word freq: \n",
    "    Robyn Speer. (2022). rspeer/wordfreq: v3.0 (v3.0.2). Zenodo. https://doi.org/10.5281/zenodo.7199437\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c67cb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lyndefolsom/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x16c662660>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import itertools\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import spacy \n",
    "\n",
    "from anagram_utils import (\n",
    "    mk_dict_from_wordnet_for_length,\n",
    "    remove_from_dict,\n",
    "    get_word_frequencies,\n",
    "    sort_words_by_frequency,\n",
    "    get_top_n_words,\n",
    "    shuffle_list,\n",
    "    reformat_sorted_wordlist,\n",
    "    check_for_doubles,\n",
    "    check_for_same,\n",
    "    find_valid_words\n",
    ")\n",
    "nltk.download(\"wordnet\") # @ russ do I need this if i've already downloaded?\n",
    "\n",
    "spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387d166f",
   "metadata": {},
   "source": [
    "<h3> Making the dictionary </h3>\n",
    "Using the utility script we make a dictionary for the words we want to use. The function mk_dict... will look for a wordlength and call up the wordnet dictionary to use for the stimuli generation. \n",
    "\n",
    "Still debugging the remove proper nouns function @to-do \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03148bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2310 words of length 4\n",
      "found 4095 words of length 5\n",
      "found 6258 words of length 6\n",
      "found after removing curse words 2307 words of length 4\n",
      "found after removing curse words 4095 words of length 5\n",
      "found after removing curse words 6258 words of length 6\n",
      "found after removing other words 2304words of length 4\n",
      "found after removing other words 4091words of length 5\n",
      "found after removing other words 6252words of length 6\n"
     ]
    }
   ],
   "source": [
    "word_dict = {}\n",
    "wordlengths = [4,5,6] # set the lengths outside of the loop bc we use them later too \n",
    "for wordlength in wordlengths:\n",
    "    word_dict[wordlength] = mk_dict_from_wordnet_for_length(wordlength)\n",
    "    print(f'found {len(word_dict[wordlength])} words of length {wordlength}')\n",
    "\n",
    "# making a loop through the word dic to use the remove proper nouns function\n",
    "# for wordlength in word_dict:\n",
    "#     word_dict[wordlength] = remove_proper_nouns(word_dict[wordlength])\n",
    "#     print(f'found after removing proper nouns {len(word_dict[wordlength])} words of length {wordlength}')\n",
    "\n",
    "# define our curse words or other words to remove\n",
    "curse_words = [\"shit\", \"piss\", \"fuck\", \"cunt\", \"cocksucker\", \"motherfucker\", \"tits\"] #rip george carlin\n",
    "other_words_to_remove = [\"jesus\", \"george\", \"john\", 'james', 'york', 'david', 'google', 'robert', 'thomas','kill','trump', 'stupid', 'centre' ]\n",
    "# use the remove from dict func to take those ones out\n",
    "for wordlength in word_dict:\n",
    "    word_dict[wordlength] = remove_from_dict(word_dict[wordlength], curse_words)\n",
    "    print(f'found after removing curse words {len(word_dict[wordlength])} words of length {wordlength}')\n",
    "\n",
    "for wordlength in word_dict:\n",
    "    word_dict[wordlength] = remove_from_dict(word_dict[wordlength], other_words_to_remove)\n",
    "    print(f'found after removing other words {len(word_dict[wordlength])}words of length {wordlength}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f187df4b",
   "metadata": {},
   "source": [
    "## Dictionary made, lets get our words\n",
    "I made a get words function to grab the list I want based off how many grams we want to have at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71eeb548",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = get_word_frequencies(word_dict) # we get the word frequencies\n",
    "sorted_wordlist = sort_words_by_frequency(word_frequencies) # we sort the dictionary by frequency\n",
    "sorted_wordlist = reformat_sorted_wordlist(sorted_wordlist) # we reformat the dictionary for the get_top_n_words function\n",
    "# make your subset list of words for the anagrams\n",
    "number_of_anagrams_per_word_length = 200\n",
    "full_list = get_top_n_words(sorted_wordlist, number_of_anagrams_per_word_length)\n",
    "# kewl so lets make a list of those words but shuffled letters ie our anagrams\n",
    "shuffled_list = shuffle_list(full_list) # we take the list and make a new list of each word's letters shuffled (anagram)\n",
    "# and for the next few things it's gonna help to also have a df of these two lits named root and shuffled\n",
    "cat_full_list = pd.DataFrame({\n",
    "    \"root\" : full_list,\n",
    "    \"shuffled\" : shuffled_list,\n",
    "    # id collumn which is a string of the index + 1 so that it's 3 digits long\n",
    "    \"id\" : [str(i+1).zfill(3) for i in range(len(full_list))]    \n",
    "})\n",
    "cat_full_list= check_for_doubles(cat_full_list) # we check for any words that are the same in the root and shuffled columns \n",
    "cat_full_list= check_for_same(cat_full_list) # we check for any duplicates in the shuffled coll and reshuffle them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882fe757",
   "metadata": {},
   "source": [
    "## Make a solution key\n",
    "\n",
    "Many anagrams have more than one response that is a valid solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bccc3150",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "slice(None, 0, None)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m anagram_by_length \u001b[38;5;241m=\u001b[39m wordlist2length_dict(shuffled_list)\n\u001b[1;32m      6\u001b[0m anagram_sets \u001b[38;5;241m=\u001b[39m distribute_words(anagram_by_length, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      7\u001b[0m anagram_sets \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup_a\u001b[39m\u001b[38;5;124m\"\u001b[39m : anagram_sets[:\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup_b\u001b[39m\u001b[38;5;124m\"\u001b[39m : anagram_sets[:\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup_c\u001b[39m\u001b[38;5;124m\"\u001b[39m : anagram_sets[:\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroup_d\u001b[39m\u001b[38;5;124m\"\u001b[39m : anagram_sets[:\u001b[38;5;241m3\u001b[39m],\n\u001b[1;32m     12\u001b[0m })\n",
      "\u001b[0;31mKeyError\u001b[0m: slice(None, 0, None)"
     ]
    }
   ],
   "source": [
    "valid_words_gram = [find_valid_words(word, word_dict) for word in shuffled_list] # we find the valid words for each anagram in the shuffled list\n",
    "valid_words = [list(set(words)) for words in valid_words_gram] # we remove any duplicates in the valid words list and set to just the one list\n",
    "cat_full_list['valid_words'] = valid_words # we add the valid words to the df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa26ca32",
   "metadata": {},
   "source": [
    "## Now we start making the stimulus files\n",
    "\n",
    "First we need to distribute the unique words among the number of groups. Groups then turn into CSV files. \n",
    "We also need to make the json file that's used for the stimulus in the experiment but I prefer it to be formated in a way that's legible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a7fe5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shbm', 'cqroa', 'uxiwsm', 'etnc', 'eslzw', 'lxngca', 'eevt', 'ummxp', 'qzmkzh', 'ngqh', 'gpnxd', 'oqlpfm', 'bkna', 'qpckv', 'cefulp', 'vxsm', 'rhqqb', 'gcypmu', 'ewdg', 'aloni', 'vbaqsk', 'mmlt', 'ebcti', 'xpfwyv', 'lsff', 'dkuml', 'pvjfqe', 'wycv', 'yanar', 'zfbgrg', 'qjpu', 'ileuw', 'bbaawq', 'iigz', 'kkwbq', 'xhscze', 'mvnq', 'xqxxo', 'mrbmyi', 'arrg', 'qplxe', 'fraina', 'rilc', 'qbkzv', 'fuarad', 'abvp', 'kpvuv', 'fagxwa', 'gfzr', 'cbadi', 'sfzdiq', 'josp', 'jxwpd', 'xnhldy', 'omic', 'rqllr', 'qnfjrv', 'kgal', 'rtfoq', 'dyxnft']\n"
     ]
    }
   ],
   "source": [
    "def wordlist2length_dict(anagram_list):\n",
    "    #  grouped by the lengths of the words\n",
    "    anagram_by_length = {}\n",
    "    for word in anagram_list:\n",
    "        length = len(word)\n",
    "        if length not in anagram_by_length:\n",
    "            anagram_by_length[length] = []\n",
    "        anagram_by_length[length].append(word)\n",
    "    return anagram_by_length\n",
    "        \n",
    "def distribute_words(anagram_by_length, num_set = 4):\n",
    "    #  distribute the words to the sets\n",
    "    anagram_sets = {}\n",
    "    for length, words in anagram_by_length.items():\n",
    "        random.shuffle(words)\n",
    "        anagram_sets[length] = [words[i::num_set] for i in range(num_set)]\n",
    "    return anagram_sets\n",
    "\n",
    "\n",
    "\n",
    "# create a test list with 20 words of length 4, 5, and 6\n",
    "import random\n",
    "import string\n",
    "\n",
    "def generate_random_word(length):\n",
    "    return ''.join(random.choices(string.ascii_lowercase, k=length))\n",
    "\n",
    "# Generate a list of 20 words with lengths 4, 5, and 6\n",
    "word_lengths = [4, 5, 6]\n",
    "test_list = [generate_random_word(length) for _ in range(20) for length in word_lengths]\n",
    "\n",
    "print(test_list)\n",
    "anagram_by_length = wordlist2length_dict(test_list)\n",
    "test_dw = distribute_words(anagram_by_length, 4)  \n",
    "\n",
    "# function to join the sets into unique collumns named group_a-d based off of the rows of the df\n",
    "def join_sets_into_groups(anagram_sets):\n",
    "    test_group = pd.DataFrame()\n",
    "    for i in range(4):\n",
    "        group = pd.DataFrame()\n",
    "        for length, words in anagram_sets.items():\n",
    "            group[f'length_{length}'] = words[i]\n",
    "        test_group = pd.concat([test_group, group], axis=1)\n",
    "    return test_group\n",
    "\n",
    "# each row of a set is joined into a new list, joined into a new df\n",
    "test_group = join_sets_into_groups(test_dw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0358a215",
   "metadata": {},
   "outputs": [],
   "source": [
    "anagram_by_length = wordlist2length_dict(shuffled_list)\n",
    "anagram_sets = distribute_words(anagram_by_length, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48814bc8",
   "metadata": {},
   "source": [
    "## New Functions\n",
    "-- distribute into groups\n",
    "-- make csv \n",
    "-- make csv turn into the json file \n",
    "-- shuffle the run order and assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53f94913",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 72\u001b[0m\n\u001b[1;32m     59\u001b[0m     set_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mrun_number\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     61\u001b[0m         {\n\u001b[1;32m     62\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: word[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m shuffled_list\n\u001b[1;32m     70\u001b[0m     ]\n\u001b[0;32m---> 72\u001b[0m test_file \u001b[38;5;241m=\u001b[39m format_js_stimuli(test_dw)\n",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m, in \u001b[0;36mformat_js_stimuli\u001b[0;34m(sets)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m word_type, word_pair \u001b[38;5;241m=\u001b[39m row\n\u001b[1;32m     26\u001b[0m original, anagram \u001b[38;5;241m=\u001b[39m word_pair\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m js_entry \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mzfill(\u001b[38;5;241m3\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Assign unique IDs starting from 001\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: word_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset\u001b[39m\u001b[38;5;124m\"\u001b[39m: set_name,\n\u001b[1;32m     33\u001b[0m }\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "# making a distributing function to spread the words across the four groups\n",
    "# def distribute_words(word_list, num_sets):\n",
    "#     # Calculate the number of words per group\n",
    "#     group_size = len(word_list) // num_sets\n",
    "\n",
    "#     # Create groups\n",
    "#     groups = [[] for _ in range(num_sets)]\n",
    "\n",
    "#     # Distribute words to each group\n",
    "#     for index, word in enumerate(word_list):\n",
    "#         group_index = index // group_size\n",
    "#         if (\n",
    "#             group_index < num_sets\n",
    "#         ):  # This check prevents index out of range if not perfectly divisible\n",
    "#             groups[group_index].append(word)\n",
    "\n",
    "#     return groups\n",
    "\n",
    "# need to mod this to take an array and convert to js format\n",
    "def format_js_stimuli(sets):\n",
    "    js_stimuli = []\n",
    "    for idx, row in enumerate(sets):\n",
    "        if idx == 0:  \n",
    "            continue\n",
    "        word_type, word_pair = row\n",
    "        original, anagram = word_pair.split(\", \")\n",
    "        js_entry = {\n",
    "            \"id\": f\"{str(idx + 1).zfill(3)}\",  # Assign unique IDs starting from 001\n",
    "            \"type\": word_type,\n",
    "            \"anagram\": anagram,\n",
    "            \"correct\": original,\n",
    "            \"set\": set_name,\n",
    "        }\n",
    "        js_entries.append(js_entry) \n",
    "    return js_stimuli\n",
    "\n",
    "\n",
    "def csv_to_js_format(csv_content, set_name):\n",
    "    js_entries = []\n",
    "    for idx, row in enumerate(csv_content):\n",
    "        if idx == 0:  # Skip header row\n",
    "            continue\n",
    "        word_type, word_pair = row\n",
    "        original, anagram = word_pair.split(\", \")\n",
    "        js_entry = {\n",
    "            \"id\": f\"{str(idx + 1).zfill(3)}\",  # Assign unique IDs starting from 001\n",
    "            \"type\": word_type,\n",
    "            \"anagram\": anagram,\n",
    "            \"correct\": original,\n",
    "            \"set\": set_name,\n",
    "        }\n",
    "        js_entries.append(js_entry)\n",
    "    return js_entries\n",
    "\n",
    "# Function to shuffle a set of words and include set run order\n",
    "def set_shuffle(word_list, set_name, run_number):\n",
    "    shuffled_list = word_list[:]  # Create a copy of the word_list to shuffle\n",
    "    random.shuffle(shuffled_list)  # Shuffle the list of words\n",
    "    set_run = f\"{set_name}{run_number}\"\n",
    "    return [\n",
    "        {\n",
    "            \"id\": word[\"id\"],\n",
    "            \"type\": word[\"type\"],\n",
    "            \"anagram\": word[\"anagram\"],\n",
    "            \"correct\": word[\"correct\"],\n",
    "            \"set\": word[\"set\"],\n",
    "            \"setRun\": set_run,\n",
    "        }\n",
    "        for word in shuffled_list\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fede517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the number of groups we will have, the groups is the total number of anagrams we have and this is called a set\n",
    "num_sets = 4\n",
    "# distribute words accross the sets and assign the set name to the row of cat_full_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e541ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the possible_words_dict into a json file in which the key is the shuffled word and the value is the list of possible words\n",
    "# all_possible_words_json = json.dumps(possible_words_dict)\n",
    "# with open(\"possible_words.json\", \"w\") as file:\n",
    "#     file.write(all_possible_words_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf7d84",
   "metadata": {},
   "source": [
    "<h3> Concatinate and create the Sets </h3>\n",
    "\n",
    "So now we need to make the stimuli for each of the blocks. For this we want to have 10 strings of each length per block for 3 blocks. \n",
    "\n",
    "And then we make groups of those until all the words are assigned. Finally, we will collect 10-30 participants per group. \n",
    "\n",
    "So in total: 3 blocks of 30 anagrams which will be 10 four letters, 10 five letters, 10 six letters. To make sure we get all the words, we will need 4 groups to have the 120 words for each string length represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3f61ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_groups = 4\n",
    "# making a distributing function to spread the words across the four groups\n",
    "def distribute_words(word_list, num_groups):\n",
    "    # Calculate the number of words per group\n",
    "    group_size = len(word_list) // num_groups\n",
    "\n",
    "    # Create groups\n",
    "    groups = [[] for _ in range(num_groups)]\n",
    "\n",
    "    # Distribute words to each group\n",
    "    for index, word in enumerate(word_list):\n",
    "        group_index = index // group_size\n",
    "        if (\n",
    "            group_index < num_groups\n",
    "        ):  # This check prevents index out of range if not perfectly divisible\n",
    "            groups[group_index].append(word)\n",
    "\n",
    "    return groups\n",
    "\n",
    "# Save each group to a separate CSV file-- we do this so we can review the words in each group and how they are distributed.\n",
    "# for i in range(4):\n",
    "#     filename = f\"group_{i+1}_word_pairs.csv\"\n",
    "#     with open(filename, \"w\", newline=\"\") as file:\n",
    "#         writer = csv.writer(file)\n",
    "#         writer.writerow([\"Type\", \"Word Pairs\"])\n",
    "#         writer.writerows(\n",
    "#             [[\"Four-Letter\", word] for word in grouped_four_letter_words[i]]\n",
    "#         )\n",
    "#         writer.writerows(\n",
    "#             [[\"Five-Letter\", word] for word in grouped_five_letter_words[i]]\n",
    "#         )\n",
    "#         writer.writerows([[\"Six-Letter\", word] for word in grouped_six_letter_words[i]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7555b9ca",
   "metadata": {},
   "source": [
    "Okay now we need to create the stimuli file which should be in a .js that will look like: \n",
    "\n",
    "let trial_objects = [\n",
    "    {\n",
    "        \"id\": \"001\",\n",
    "        \"type\": \"Four-Letter\",\n",
    "        \"anagram\": \"atth\",\n",
    "        \"correct\": \"that\",\n",
    "        \"set\": \"A\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bba816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_directory = (\n",
    "#     \"./group_1_word_pairs.csv\",\n",
    "#     \"./group_2_word_pairs.csv\",\n",
    "#     \"./group_3_word_pairs.csv\",\n",
    "#     \"./group_4_word_pairs.csv\",\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# Placeholder for all JS entries\n",
    "all_entries = []\n",
    "\n",
    "# Loop over each CSV file and set name\n",
    "for i in range(4):\n",
    "    filename = f\"group_{i+1}_word_pairs.csv\"\n",
    "    set_name = f\"Set{chr(65 + i)}\"  # 'SetA', 'SetB', 'SetC', 'SetD'\n",
    "\n",
    "    with open(filename, newline=\"\") as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        js_entries = csv_to_js_format(\n",
    "            csvreader, set_name\n",
    "        )  # Pass csvreader and set_name\n",
    "        all_entries.extend(js_entries)\n",
    "\n",
    "## Writing the JS file\n",
    "# Save all entries into the JS file\n",
    "stimuli_js_content = \"let trial_objects = \" + str(all_entries) + \";\"\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(stimuli_js_content)\n",
    "\n",
    "# modfiying the stimuli.js file to be in the preferred format (mostly for readability)\n",
    "# Here we format the JSON with specific spacing and bracketing style\n",
    "stimuli_js_content = \"let trial_objects = [\\n\"\n",
    "for entry in all_entries:\n",
    "    stimuli_js_content += \"    \" + json.dumps(entry, indent=4) + \",\\n\"\n",
    "stimuli_js_content = stimuli_js_content.rstrip(\",\\n\") + \"\\n];\"\n",
    "\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(stimuli_js_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a2c41",
   "metadata": {},
   "source": [
    "Alright we've now got a set but we are gonna take each set and then shuffle into 4 coded runs.\n",
    "Essentially, by trying to randomize the order on the fly, we can introduce a bunch of bugs into the timeline variables. \n",
    "By hardcoding 4 unique run orders for each set of words, we are addressing order effects without risking more bugs. \n",
    "\n",
    "So below we take the stimuli file, filter by set, add a new parameter to the stimuli file which is its run order assignment which is numbered 1-4. \n",
    "Now ALL of the words in set A will be shuffled into 4 unique orders and assigned to A1, A2, A3, A4. All the A words are the same but their order is now randomized.\n",
    "The this will be saved into the stimuli file as \"SetRun\":\"A1\" etc etc.\n",
    "\n",
    "Now this will make the Stimuli.js file increase substancially (by four) but will keep our code clean and modular. Also fewer headaches since the other option is adjusting the JS utility file that makes the variable order and we really don't wanna do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afebca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Placeholder for all JS entries\n",
    "all_entries = []\n",
    "\n",
    "# Loop over each CSV file and set name\n",
    "for i in range(4):\n",
    "    filename = f\"group_{i+1}_word_pairs.csv\"\n",
    "    set_name = f\"Set{chr(65 + i)}\"  # 'SetA', 'SetB', 'SetC', 'SetD'\n",
    "\n",
    "    with open(filename, newline=\"\") as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        csv_content = list(csvreader)  # Convert csvreader to a list\n",
    "        js_entries = csv_to_js_format(\n",
    "            csv_content, set_name\n",
    "        )  # Pass csv_content and set_name\n",
    "        for run_number in range(1, 5):  # Create 4 runs for each set\n",
    "            shuffled_set = set_shuffle(js_entries, set_name, run_number)\n",
    "            all_entries.extend(shuffled_set)\n",
    "\n",
    "# Save all entries into the JS file\n",
    "stimuli_js_content = \"let trial_objects = \" + json.dumps(all_entries, indent=4) + \";\"\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(stimuli_js_content)\n",
    "\n",
    "# Count occurrences of each id... okay well this isn't right but also not really important, so I'll leave it as is. What's happening is that the id is being counted but the assignment is made during a loop and so for each id there is 4 unique string pairs. for example 032 is about, water, birth, and trust.\n",
    "id_counter = Counter(entry[\"id\"] for entry in all_entries)\n",
    "print(\"ID Occurrences:\")\n",
    "for id_, count in id_counter.items():\n",
    "    print(f\"id {id_}: {count} times\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caac0b83",
   "metadata": {},
   "source": [
    "Okay I'm just looking for a sanity check and gonna run a loop over the stimuli js file to correct the  ID and then see if that does it. I think this notebook should be a code review for lab someday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0785e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the stimuli file\n",
    "with open(\"stimuli.js\", \"r\") as file:\n",
    "    stimuli_js_content = file.read()\n",
    "\n",
    "# Extract the JSON data from the stimuli file\n",
    "json_data = json.loads(stimuli_js_content[len(\"let trial_objects = \") : -1])\n",
    "\n",
    "# Create a mapping for unique anagrams to new IDs\n",
    "unique_anagram_to_id = {}\n",
    "id_counter = 1\n",
    "\n",
    "for entry in json_data:\n",
    "    anagram = entry[\"anagram\"]\n",
    "    if anagram not in unique_anagram_to_id:\n",
    "        unique_anagram_to_id[anagram] = f\"{id_counter:03d}\"\n",
    "        id_counter += 1\n",
    "\n",
    "# Reassign IDs in the JSON data\n",
    "for entry in json_data:\n",
    "    entry[\"id\"] = unique_anagram_to_id[entry[\"anagram\"]]\n",
    "\n",
    "# Save the updated entries into the JS file\n",
    "updated_stimuli_js_content = (\n",
    "    \"let trial_objects = \" + json.dumps(json_data, indent=4) + \";\"\n",
    ")\n",
    "with open(\"updated_stimuli.js\", \"w\") as file:\n",
    "    file.write(updated_stimuli_js_content)\n",
    "\n",
    "# Print the new ID mapping for verification\n",
    "print(\"New ID mapping for unique anagrams:\")\n",
    "for anagram, new_id in unique_anagram_to_id.items():\n",
    "    print(f\"Anagram: {anagram}, New ID: {new_id}\")\n",
    "\n",
    "# This worked okay lets save to the stimuli.js file\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(updated_stimuli_js_content)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python (miniconda3)",
   "language": "python",
   "name": "miniconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
