{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cca7df1d",
   "metadata": {},
   "source": [
    "<h2> Generating the Stimuli file</h2>\n",
    "    \n",
    "This notebook is a modified version of prior work to generate the stimuli files used in anagram experiments.\n",
    "\n",
    "First we grab a list of words and then shuffle. From the shuffled set we then set up seperate groups (set groups) from there we use those paired shuffle and \"correct\" words to make the stimuli file used in the anagram experiments. \n",
    "\n",
    "This one will differ since I'll be including the sources and frequency data on use of the words in the set. Furthermore, I will be using this notebook to also make a json object of valid solutions for each shuffle. The stimuli file we use includes the word used for shuffling however, those shuffled strings can be solved to more than one real english word which is how we define \"valid\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ce3545",
   "metadata": {},
   "source": [
    "We can use dictionaries that have all the words of specific strings than use a function to give us some number of those words. \n",
    "\n",
    "#### The word bank we are using is from Word Net:\n",
    "    George A. Miller (1995). WordNet: A Lexical Database for English.\n",
    "    Communications of the ACM Vol. 38, No. 11: 39-41.\n",
    "    Christiane Fellbaum (1998, ed.) WordNet: An Electronic Lexical Database. Cambridge, MA: MIT Press.\n",
    "    WordNet: An Electronic Lexical Database\n",
    "\n",
    "#### The frequency information is from the word freq: \n",
    "    Robyn Speer. (2022). rspeer/wordfreq: v3.0 (v3.0.2). Zenodo. https://doi.org/10.5281/zenodo.7199437\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c67cb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lyndefolsom/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x30136a0f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import itertools\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import spacy \n",
    "\n",
    "from propernoun import remove_proper_nouns\n",
    "from anagram_utils import (\n",
    "    mk_dict_from_wordnet_for_length,\n",
    "    remove_from_dict,\n",
    "    get_word_frequencies,\n",
    "    sort_words_by_frequency,\n",
    "    get_top_n_words,\n",
    "    shuffle_letters,# @ russ do I keep this if it is used in the shuffle list function? \n",
    "    shuffle_list,\n",
    "    reformat_sorted_wordlist,\n",
    "    check_for_doubles,\n",
    "    check_for_same\n",
    ")\n",
    "nltk.download(\"wordnet\") # @ russ do I need this if i've already downloaded?\n",
    "\n",
    "spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e446f8",
   "metadata": {},
   "source": [
    "A code chunk for script testing (bc lynde is scared of using their terminal and messing with their enviornment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387d166f",
   "metadata": {},
   "source": [
    "<h3> Making the dictionary </h3>\n",
    "Using the utility script we make a dictionary for the words we want to use. The function mk_dict... will look for a wordlength and call up the wordnet dictionary to use for the stimuli generation. \n",
    "\n",
    "Still debugging the remove proper nouns function @to-do \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03148bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2310 words of length 4\n",
      "found 4095 words of length 5\n",
      "found 6258 words of length 6\n",
      "found after removing proper nouns 2234 words of length 4\n",
      "found after removing proper nouns 3982 words of length 5\n",
      "found after removing proper nouns 6071 words of length 6\n",
      "found after removing curse words 2231 words of length 4\n",
      "found after removing curse words 3982 words of length 5\n",
      "found after removing curse words 6071 words of length 6\n",
      "found after removing other words 2228words of length 4\n",
      "found after removing other words 3978words of length 5\n",
      "found after removing other words 6066words of length 6\n"
     ]
    }
   ],
   "source": [
    "word_dict = {}\n",
    "wordlengths = [4,5,6] # set the lengths outside of the loop bc we use them later too \n",
    "for wordlength in wordlengths:\n",
    "    word_dict[wordlength] = mk_dict_from_wordnet_for_length(wordlength)\n",
    "    print(f'found {len(word_dict[wordlength])} words of length {wordlength}')\n",
    "\n",
    "# making a loop through the word dic to use the remove proper nouns function\n",
    "for wordlength in word_dict:\n",
    "    word_dict[wordlength] = remove_proper_nouns(word_dict[wordlength])\n",
    "    print(f'found after removing proper nouns {len(word_dict[wordlength])} words of length {wordlength}')\n",
    "\n",
    "# define our curse words or other words to remove\n",
    "curse_words = [\"shit\", \"piss\", \"fuck\", \"cunt\", \"cocksucker\", \"motherfucker\", \"tits\"] #rip george carlin\n",
    "other_words_to_remove = [\"jesus\", \"george\", \"john\", 'james', 'york', 'david', 'google', 'robert', 'thomas','kill','trump', 'stupid' ]\n",
    "# use the remove from dict func to take those ones out\n",
    "for wordlength in word_dict:\n",
    "    word_dict[wordlength] = remove_from_dict(word_dict[wordlength], curse_words)\n",
    "    print(f'found after removing curse words {len(word_dict[wordlength])} words of length {wordlength}')\n",
    "\n",
    "for wordlength in word_dict:\n",
    "    word_dict[wordlength] = remove_from_dict(word_dict[wordlength], other_words_to_remove)\n",
    "    print(f'found after removing other words {len(word_dict[wordlength])}words of length {wordlength}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f187df4b",
   "metadata": {},
   "source": [
    "## Dictionary made, lets get our words\n",
    "I made a get words function to grab the list I want based off how many grams we want to have at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71eeb548",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = get_word_frequencies(word_dict) # we get the word frequencies\n",
    "sorted_wordlist = sort_words_by_frequency(word_frequencies) # we sort the dictionary by frequency\n",
    "sorted_wordlist = reformat_sorted_wordlist(sorted_wordlist) # we reformat the dictionary for the get_top_n_words function\n",
    "# make your subset list of words for the anagrams\n",
    "number_of_anagrams_per_word_length = 200\n",
    "full_list = get_top_n_words(sorted_wordlist, number_of_anagrams_per_word_length)\n",
    "# kewl so lets make a list of those words but shuffled letters ie our anagrams\n",
    "shuffled_list = shuffle_list(full_list) # we take the list and make a new list of each word's letters shuffled (anagram)\n",
    "# and for the next few things it's gonna help to also have a df of these two lits named root and shuffled\n",
    "cat_full_list = pd.DataFrame({\n",
    "    \"root\" : full_list,\n",
    "    \"shuffled\" : shuffled_list #must name it shuffled for the next function\n",
    "})\n",
    "cat_full_list= check_for_doubles(cat_full_list) # we check for any words that are the same in the root and shuffled columns \n",
    "cat_full_list= check_for_same(cat_full_list) # we check for any duplicates in the shuffled coll and reshuffle them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9d1703",
   "metadata": {},
   "source": [
    "# New functions\n",
    "Okay I'm realizing that I've gotta make more functions (RIP). I draft them here and then move them to the Utils file. \n",
    "After the util file, I'll need to write a test (double RIP). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a39620e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m      \u001b[38;5;28;01mreturn\u001b[39;00m valid_words\n\u001b[1;32m     19\u001b[0m test_strings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meltie\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdcoe\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcowr\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearb\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 20\u001b[0m test_strings_valid_words \u001b[38;5;241m=\u001b[39m find_valid_words(test_strings)\n",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m, in \u001b[0;36mfind_valid_words\u001b[0;34m(input_string)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m perm \u001b[38;5;129;01min\u001b[39;00m permutations:\n\u001b[1;32m     13\u001b[0m     perm_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(perm) \u001b[38;5;66;03m# @lynde todo, fix this\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     result \u001b[38;5;241m=\u001b[39m check_word(perm_word)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis not in the dictionary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[1;32m     16\u001b[0m         valid_words\u001b[38;5;241m.\u001b[39mappend(perm_word)\n",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m, in \u001b[0;36mcheck_word\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_word\u001b[39m(word):\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m word_dict[wordlengths]\u001b[38;5;241m.\u001b[39mvalues:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is in the dictionary\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "#function to check if the word is in the dictionary (needed for function below)\n",
    "def check_word(word):\n",
    "    if word in word_dict[wordlengths].values:\n",
    "        return f\"{word} is in the dictionary\"\n",
    "    else:\n",
    "        return f\"{word} is not in the dictionary\"\n",
    "    \n",
    "#function for finding the possible words made out of the string of letters in shuffled, we are calling these the valid words\n",
    "def find_valid_words(input_string):\n",
    "     valid_words = []\n",
    "     permutations = itertools.permutations(input_string)\n",
    "     for perm in permutations:\n",
    "         perm_word = \"\".join(perm) # @lynde todo, fix this\n",
    "         result = check_word(perm_word)\n",
    "         if \"is not in the dictionary\" not in result:\n",
    "             valid_words.append(perm_word)\n",
    "     return valid_words\n",
    "\n",
    "test_strings = ['eltie','dcoe','cowr','earb']\n",
    "test_strings_valid_words = find_valid_words(test_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07112780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "882fe757",
   "metadata": {},
   "source": [
    "## Make a solution key\n",
    "\n",
    "Many anagrams have more than one response that is a valid solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd284b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_valid_words(input_string):\n",
    "#     valid_words = []\n",
    "#     permutations = itertools.permutations(input_string)\n",
    "\n",
    "#     for perm in permutations:\n",
    "#         perm_word = \"\".join(perm)\n",
    "#         result = check_word(perm_word)\n",
    "#         if \"is not in the dictionary\" not in result:\n",
    "#             valid_words.append(perm_word)\n",
    "\n",
    "#     return valid_words\n",
    "\n",
    "\n",
    "# now we go through the list of shuffled words use the find_valid_words for each word and append the results to a list\n",
    "# five_possible_words = []\n",
    "# for word in shuffled_five:\n",
    "#     possible_words = find_valid_words(word)\n",
    "#     five_possible_words.append(possible_words)\n",
    "# six_possible_words = []\n",
    "# for word in shuffled_six:\n",
    "#     possible_words = find_valid_words(word)\n",
    "#     six_possible_words.append(possible_words)\n",
    "# four_possible_words = []\n",
    "# for word in shuffled_fours:\n",
    "#     possible_words = find_valid_words(word)\n",
    "#     four_possible_words.append(possible_words)\n",
    "\n",
    "# # aggregate all of these lists into one list\n",
    "# all_possible_words = five_possible_words + six_possible_words + four_possible_words\n",
    "# # convert the list into a dictionary with the shuffled word as the key and the list of possible words as the value\n",
    "# possible_words_dict = dict(\n",
    "#     zip(shuffled_five + shuffled_six + shuffled_fours, all_possible_words)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa26ca32",
   "metadata": {},
   "source": [
    "## With an All Possible file we can check valid\n",
    "Now that we have all the possible words that can be made from a particular string, we can use that dictionary as a json that could be read into the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e541ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the possible_words_dict into a json file in which the key is the shuffled word and the value is the list of possible words\n",
    "# all_possible_words_json = json.dumps(possible_words_dict)\n",
    "# with open(\"possible_words.json\", \"w\") as file:\n",
    "#     file.write(all_possible_words_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf7d84",
   "metadata": {},
   "source": [
    "<h3> Concatinate and create the Sets </h3>\n",
    "\n",
    "So now we need to make the stimuli for each of the blocks. For this we want to have 10 strings of each length per block for 3 blocks. \n",
    "\n",
    "And then we make groups of those until all the words are assigned. Finally, we will collect 10-30 participants per group. \n",
    "\n",
    "So in total: 3 blocks of 30 anagrams which will be 10 four letters, 10 five letters, 10 six letters. To make sure we get all the words, we will need 4 groups to have the 120 words for each string length represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3f61ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a distributing function to spread the words across the four groups\n",
    "def distribute_words(word_list, num_groups=4):\n",
    "    # Calculate the number of words per group\n",
    "    group_size = len(word_list) // num_groups\n",
    "\n",
    "    # Create groups\n",
    "    groups = [[] for _ in range(num_groups)]\n",
    "\n",
    "    # Distribute words to each group\n",
    "    for index, word in enumerate(word_list):\n",
    "        group_index = index // group_size\n",
    "        if (\n",
    "            group_index < num_groups\n",
    "        ):  # This check prevents index out of range if not perfectly divisible\n",
    "            groups[group_index].append(word)\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "# Distribute words to the groups\n",
    "# grouped_four_letter_words = distribute_words(shuffled_four_pairs)\n",
    "# grouped_five_letter_words = distribute_words(shuffled_pairs_five)\n",
    "# grouped_six_letter_words = distribute_words(shuffled_six_pairs)\n",
    "\n",
    "# Save each group to a separate CSV file-- we do this so we can review the words in each group and how they are distributed.\n",
    "# for i in range(4):\n",
    "#     filename = f\"group_{i+1}_word_pairs.csv\"\n",
    "#     with open(filename, \"w\", newline=\"\") as file:\n",
    "#         writer = csv.writer(file)\n",
    "#         writer.writerow([\"Type\", \"Word Pairs\"])\n",
    "#         writer.writerows(\n",
    "#             [[\"Four-Letter\", word] for word in grouped_four_letter_words[i]]\n",
    "#         )\n",
    "#         writer.writerows(\n",
    "#             [[\"Five-Letter\", word] for word in grouped_five_letter_words[i]]\n",
    "#         )\n",
    "#         writer.writerows([[\"Six-Letter\", word] for word in grouped_six_letter_words[i]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7555b9ca",
   "metadata": {},
   "source": [
    "Okay now we need to create the stimuli file which should be in a .js that will look like: \n",
    "\n",
    "let trial_objects = [\n",
    "    {\n",
    "        \"id\": \"001\",\n",
    "        \"type\": \"Four-Letter\",\n",
    "        \"anagram\": \"atth\",\n",
    "        \"correct\": \"that\",\n",
    "        \"set\": \"A\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bba816",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_directory = (\n",
    "    \"./group_1_word_pairs.csv\",\n",
    "    \"./group_2_word_pairs.csv\",\n",
    "    \"./group_3_word_pairs.csv\",\n",
    "    \"./group_4_word_pairs.csv\",\n",
    ")\n",
    "\n",
    "# Function to convert CSV content to JS format\n",
    "def csv_to_js_format(csv_content, set_name):\n",
    "    js_entries = []\n",
    "    for idx, row in enumerate(csv_content):\n",
    "        word_type, word_pair = row\n",
    "        if idx == 0:  # Skip header row\n",
    "            continue\n",
    "        original, anagram = word_pair.split(\", \")\n",
    "        js_entry = {\n",
    "            \"id\": f\"{str(idx).zfill(3)}\",\n",
    "            \"type\": word_type,\n",
    "            \"anagram\": anagram,\n",
    "            \"correct\": original,\n",
    "            \"set\": set_name,\n",
    "        }\n",
    "        js_entries.append(js_entry)\n",
    "    return js_entries\n",
    "\n",
    "\n",
    "# Placeholder for all JS entries\n",
    "all_entries = []\n",
    "\n",
    "# Loop over each CSV file and set name\n",
    "for i in range(4):\n",
    "    filename = f\"group_{i+1}_word_pairs.csv\"\n",
    "    set_name = f\"Set{chr(65 + i)}\"  # 'SetA', 'SetB', 'SetC', 'SetD'\n",
    "\n",
    "    with open(filename, newline=\"\") as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        js_entries = csv_to_js_format(\n",
    "            csvreader, set_name\n",
    "        )  # Pass csvreader and set_name\n",
    "        all_entries.extend(js_entries)\n",
    "\n",
    "## Writing the JS file\n",
    "# Save all entries into the JS file\n",
    "stimuli_js_content = \"let trial_objects = \" + str(all_entries) + \";\"\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(stimuli_js_content)\n",
    "\n",
    "# modfiying the stimuli.js file to be in the preferred format (mostly for readability)\n",
    "# Here we format the JSON with specific spacing and bracketing style\n",
    "stimuli_js_content = \"let trial_objects = [\\n\"\n",
    "for entry in all_entries:\n",
    "    stimuli_js_content += \"    \" + json.dumps(entry, indent=4) + \",\\n\"\n",
    "stimuli_js_content = stimuli_js_content.rstrip(\",\\n\") + \"\\n];\"\n",
    "\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(stimuli_js_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a2c41",
   "metadata": {},
   "source": [
    "Alright we've now got a set but we are gonna take each set and then shuffle into 4 coded runs.\n",
    "Essentially, by trying to randomize the order on the fly, we can introduce a bunch of bugs into the timeline variables. \n",
    "By hardcoding 4 unique run orders for each set of words, we are addressing order effects without risking more bugs. \n",
    "\n",
    "So below we take the stimuli file, filter by set, add a new parameter to the stimuli file which is its run order assignment which is numbered 1-4. \n",
    "Now ALL of the words in set A will be shuffled into 4 unique orders and assigned to A1, A2, A3, A4. All the A words are the same but their order is now randomized.\n",
    "The this will be saved into the stimuli file as \"SetRun\":\"A1\" etc etc.\n",
    "\n",
    "Now this will make the Stimuli.js file increase substancially (by four) but will keep our code clean and modular. Also fewer headaches since the other option is adjusting the JS utility file that makes the variable order and we really don't wanna do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afebca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert CSV content to JS format\n",
    "def csv_to_js_format(csv_content, set_name):\n",
    "    js_entries = []\n",
    "    for idx, row in enumerate(csv_content):\n",
    "        if idx == 0:  # Skip header row\n",
    "            continue\n",
    "        word_type, word_pair = row\n",
    "        original, anagram = word_pair.split(\", \")\n",
    "        js_entry = {\n",
    "            \"id\": f\"{str(idx + 1).zfill(3)}\",  # Assign unique IDs starting from 001\n",
    "            \"type\": word_type,\n",
    "            \"anagram\": anagram,\n",
    "            \"correct\": original,\n",
    "            \"set\": set_name,\n",
    "        }\n",
    "        js_entries.append(js_entry)\n",
    "    return js_entries\n",
    "\n",
    "\n",
    "# Function to shuffle a set of words and include set run order\n",
    "def set_shuffle(word_list, set_name, run_number):\n",
    "    shuffled_list = word_list[:]  # Create a copy of the word_list to shuffle\n",
    "    random.shuffle(shuffled_list)  # Shuffle the list of words\n",
    "    set_run = f\"{set_name}{run_number}\"\n",
    "    return [\n",
    "        {\n",
    "            \"id\": word[\"id\"],\n",
    "            \"type\": word[\"type\"],\n",
    "            \"anagram\": word[\"anagram\"],\n",
    "            \"correct\": word[\"correct\"],\n",
    "            \"set\": word[\"set\"],\n",
    "            \"setRun\": set_run,\n",
    "        }\n",
    "        for word in shuffled_list\n",
    "    ]\n",
    "\n",
    "\n",
    "# Placeholder for all JS entries\n",
    "all_entries = []\n",
    "\n",
    "# Loop over each CSV file and set name\n",
    "for i in range(4):\n",
    "    filename = f\"group_{i+1}_word_pairs.csv\"\n",
    "    set_name = f\"Set{chr(65 + i)}\"  # 'SetA', 'SetB', 'SetC', 'SetD'\n",
    "\n",
    "    with open(filename, newline=\"\") as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        csv_content = list(csvreader)  # Convert csvreader to a list\n",
    "        js_entries = csv_to_js_format(\n",
    "            csv_content, set_name\n",
    "        )  # Pass csv_content and set_name\n",
    "        for run_number in range(1, 5):  # Create 4 runs for each set\n",
    "            shuffled_set = set_shuffle(js_entries, set_name, run_number)\n",
    "            all_entries.extend(shuffled_set)\n",
    "\n",
    "# Save all entries into the JS file\n",
    "stimuli_js_content = \"let trial_objects = \" + json.dumps(all_entries, indent=4) + \";\"\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(stimuli_js_content)\n",
    "\n",
    "# Count occurrences of each id... okay well this isn't right but also not really important, so I'll leave it as is. What's happening is that the id is being counted but the assignment is made during a loop and so for each id there is 4 unique string pairs. for example 032 is about, water, birth, and trust.\n",
    "id_counter = Counter(entry[\"id\"] for entry in all_entries)\n",
    "print(\"ID Occurrences:\")\n",
    "for id_, count in id_counter.items():\n",
    "    print(f\"id {id_}: {count} times\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caac0b83",
   "metadata": {},
   "source": [
    "Okay I'm just looking for a sanity check and gonna run a loop over the stimuli js file to correct the  ID and then see if that does it. I think this notebook should be a code review for lab someday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0785e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the stimuli file\n",
    "with open(\"stimuli.js\", \"r\") as file:\n",
    "    stimuli_js_content = file.read()\n",
    "\n",
    "# Extract the JSON data from the stimuli file\n",
    "json_data = json.loads(stimuli_js_content[len(\"let trial_objects = \") : -1])\n",
    "\n",
    "# Create a mapping for unique anagrams to new IDs\n",
    "unique_anagram_to_id = {}\n",
    "id_counter = 1\n",
    "\n",
    "for entry in json_data:\n",
    "    anagram = entry[\"anagram\"]\n",
    "    if anagram not in unique_anagram_to_id:\n",
    "        unique_anagram_to_id[anagram] = f\"{id_counter:03d}\"\n",
    "        id_counter += 1\n",
    "\n",
    "# Reassign IDs in the JSON data\n",
    "for entry in json_data:\n",
    "    entry[\"id\"] = unique_anagram_to_id[entry[\"anagram\"]]\n",
    "\n",
    "# Save the updated entries into the JS file\n",
    "updated_stimuli_js_content = (\n",
    "    \"let trial_objects = \" + json.dumps(json_data, indent=4) + \";\"\n",
    ")\n",
    "with open(\"updated_stimuli.js\", \"w\") as file:\n",
    "    file.write(updated_stimuli_js_content)\n",
    "\n",
    "# Print the new ID mapping for verification\n",
    "print(\"New ID mapping for unique anagrams:\")\n",
    "for anagram, new_id in unique_anagram_to_id.items():\n",
    "    print(f\"Anagram: {anagram}, New ID: {new_id}\")\n",
    "\n",
    "# This worked okay lets save to the stimuli.js file\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(updated_stimuli_js_content)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python (miniconda3)",
   "language": "python",
   "name": "miniconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
