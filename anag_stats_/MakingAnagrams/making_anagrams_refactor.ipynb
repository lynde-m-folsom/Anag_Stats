{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cca7df1d",
   "metadata": {},
   "source": [
    "<h2> Generating the Stimuli file</h2>\n",
    "    \n",
    "This notebook is a modified version of prior work to generate the stimuli files used in anagram experiments.\n",
    "\n",
    "First we grab a list of words and then shuffle. From the shuffled set we then set up seperate groups (set groups) from there we use those paired shuffle and \"correct\" words to make the stimuli file used in the anagram experiments. \n",
    "\n",
    "This one will differ since I'll be including the sources and frequency data on use of the words in the set. Furthermore, I will be using this notebook to also make a json object of valid solutions for each shuffle. The stimuli file we use includes the word used for shuffling however, those shuffled strings can be solved to more than one real english word which is how we define \"valid\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ce3545",
   "metadata": {},
   "source": [
    "We can use dictionaries that have all the words of specific strings than use a function to give us some number of those words. \n",
    "\n",
    "#### The word bank we are using is from Word Net:\n",
    "    George A. Miller (1995). WordNet: A Lexical Database for English.\n",
    "    Communications of the ACM Vol. 38, No. 11: 39-41.\n",
    "    Christiane Fellbaum (1998, ed.) WordNet: An Electronic Lexical Database. Cambridge, MA: MIT Press.\n",
    "    WordNet: An Electronic Lexical Database\n",
    "\n",
    "#### The frequency information is from the word freq: \n",
    "    Robyn Speer. (2022). rspeer/wordfreq: v3.0 (v3.0.2). Zenodo. https://doi.org/10.5281/zenodo.7199437\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c67cb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lyndefolsom/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x16ad34c80>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import itertools\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import spacy \n",
    "\n",
    "from anagram_utils import (\n",
    "    mk_dict_from_wordnet_for_length,\n",
    "    remove_from_dict,\n",
    "    get_word_frequencies,\n",
    "    sort_words_by_frequency,\n",
    "    get_top_n_words,\n",
    "    shuffle_list,\n",
    "    reformat_sorted_wordlist,\n",
    "    check_for_word,\n",
    "    check_for_doubles,\n",
    "    check_for_same,\n",
    "    find_valid_words\n",
    ")\n",
    "nltk.download(\"wordnet\") # @ russ do I need this if i've already downloaded?\n",
    "\n",
    "spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387d166f",
   "metadata": {},
   "source": [
    "<h3> Making the dictionary </h3>\n",
    "Using the utility script we make a dictionary for the words we want to use. The function mk_dict... will look for a wordlength and call up the wordnet dictionary to use for the stimuli generation. \n",
    "\n",
    "Still debugging the remove proper nouns function @to-do \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03148bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2310 words of length 4\n",
      "found 4095 words of length 5\n",
      "found 6258 words of length 6\n",
      "found after removing curse words 2307 words of length 4\n",
      "found after removing curse words 4095 words of length 5\n",
      "found after removing curse words 6258 words of length 6\n",
      "found after removing other words 2304words of length 4\n",
      "found after removing other words 4091words of length 5\n",
      "found after removing other words 6252words of length 6\n"
     ]
    }
   ],
   "source": [
    "word_dict = {}\n",
    "wordlengths = [4,5,6] # set the lengths outside of the loop bc we use them later too \n",
    "for wordlength in wordlengths:\n",
    "    word_dict[wordlength] = mk_dict_from_wordnet_for_length(wordlength)\n",
    "    print(f'found {len(word_dict[wordlength])} words of length {wordlength}')\n",
    "\n",
    "# making a loop through the word dic to use the remove proper nouns function\n",
    "# for wordlength in word_dict:\n",
    "#     word_dict[wordlength] = remove_proper_nouns(word_dict[wordlength])\n",
    "#     print(f'found after removing proper nouns {len(word_dict[wordlength])} words of length {wordlength}')\n",
    "\n",
    "# define our curse words or other words to remove\n",
    "curse_words = [\"shit\", \"piss\", \"fuck\", \"cunt\", \"cocksucker\", \"motherfucker\", \"tits\"] #rip george carlin\n",
    "other_words_to_remove = [\"jesus\", \"george\", \"john\", 'james', 'york', 'david', 'google', 'robert', 'thomas','kill','trump', 'stupid', 'centre' ]\n",
    "# use the remove from dict func to take those ones out\n",
    "for wordlength in word_dict:\n",
    "    word_dict[wordlength] = remove_from_dict(word_dict[wordlength], curse_words)\n",
    "    print(f'found after removing curse words {len(word_dict[wordlength])} words of length {wordlength}')\n",
    "\n",
    "for wordlength in word_dict:\n",
    "    word_dict[wordlength] = remove_from_dict(word_dict[wordlength], other_words_to_remove)\n",
    "    print(f'found after removing other words {len(word_dict[wordlength])}words of length {wordlength}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f187df4b",
   "metadata": {},
   "source": [
    "## Dictionary made, lets get our words\n",
    "I made a get words function to grab the list I want based off how many grams we want to have at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71eeb548",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = get_word_frequencies(word_dict) # we get the word frequencies\n",
    "sorted_wordlist = sort_words_by_frequency(word_frequencies) # we sort the dictionary by frequency\n",
    "sorted_wordlist = reformat_sorted_wordlist(sorted_wordlist) # we reformat the dictionary for the get_top_n_words function\n",
    "# make your subset list of words for the anagrams\n",
    "number_of_anagrams_per_word_length = 200\n",
    "full_list = get_top_n_words(sorted_wordlist, number_of_anagrams_per_word_length)\n",
    "# kewl so lets make a list of those words but shuffled letters ie our anagrams\n",
    "shuffled_list = shuffle_list(full_list) # we take the list and make a new list of each word's letters shuffled (anagram)\n",
    "# and for the next few things it's gonna help to also have a df of these two lits named root and shuffled\n",
    "cat_full_list = pd.DataFrame({\n",
    "    \"root\" : full_list,\n",
    "    \"shuffled\" : shuffled_list,\n",
    "    # id collumn which is a string of the index + 1 so that it's 3 digits long\n",
    "    \"id\" : [str(i+1).zfill(3) for i in range(len(full_list))]    \n",
    "})\n",
    "cat_full_list= check_for_doubles(cat_full_list) # we check for any words that are the same in the root and shuffled columns \n",
    "cat_full_list= check_for_same(cat_full_list) # we check for any duplicates in the shuffled coll and reshuffle them\n",
    "## need to make a function that checks that words that are \"shuffled\" do not exist in the dictionary, then shuffle if they are\n",
    "cat_full_list = check_for_word(cat_full_list, word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882fe757",
   "metadata": {},
   "source": [
    "## Make a solution key\n",
    "\n",
    "Many anagrams have more than one response that is a valid solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bccc3150",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_words_gram = [find_valid_words(word, word_dict) for word in shuffled_list] # we find the valid words for each anagram in the shuffled list\n",
    "valid_words = [list(set(words)) for words in valid_words_gram] # we remove any duplicates in the valid words list and set to just the one list\n",
    "cat_full_list['valid_words'] = valid_words # we add the valid words to the df\n",
    "cat_full_list=check_for_word(cat_full_list, word_dict) # we check for any words that are the same in the root and shuffled columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c8039e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SetA has 150 words\n",
      "SetB has 150 words\n",
      "SetC has 150 words\n",
      "SetD has 150 words\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# function to use the root in the df to assign it's string length in a new column of the same def\n",
    "def get_length(row):\n",
    "    return len(row['root']) \n",
    "\n",
    "\n",
    "# apply the function to the df\n",
    "cat_full_list['length'] = cat_full_list.apply(get_length, axis=1)\n",
    "\n",
    "# function to first group by the length in a DF then randomly assign to a group a through d \n",
    "def group_by_length(df):\n",
    "    #\n",
    "    if len(df) % 4 != 0:\n",
    "        raise \"Error in distributing between 4 groups, please revise counts\"\n",
    "    # group by the length of the word\n",
    "    grouped = df.groupby('length')\n",
    "    # make a list of the groups\n",
    "    groups = [group for name, group in grouped]\n",
    "    # make a list of the group names\n",
    "    group_names = ['SetA', 'SetB', 'SetC', 'SetD']\n",
    "    # calculate number of words per group\n",
    "    num_words_per_group = len(df) // len(group_names)\n",
    "    # make a dictionary of the group names and the groups\n",
    "    group_dict = dict(zip(group_names, groups))\n",
    "    # make a list of the group names for each word in the df don't exceed the num words per group\n",
    "    group_list = [] # make an empty list to append to\n",
    "    # loop through the group names until the number of words in the group is equal to the number of words per group\n",
    "    for group_name in group_names:\n",
    "        for i in range(num_words_per_group):\n",
    "            group_list.append(group_name)\n",
    "    # assign the group list to the df\n",
    "    df['Set'] = group_list\n",
    "    return df\n",
    "\n",
    "# apply the function to the df\n",
    "cat_full_list = group_by_length(cat_full_list)\n",
    "\n",
    "# check that each group in the df has the same number of words\n",
    "grouped = cat_full_list.groupby('Set')\n",
    "for name, group in grouped:\n",
    "    print(f'{name} has {len(group)} words')\n",
    "\n",
    "#### troublesome function ***** now we create 4 unique set runs for each set and call that col setrun and it uses the val of set and adds it's number\n",
    "### so that we have 4 unique runs for each set, this would then mean our final js stim  would have 16 unique runs, 4 for each set, and a file that ought to be len(id) * 16\n",
    "\n",
    "def set_run(df):\n",
    "    # group by set\n",
    "    grouped = df.groupby('Set')\n",
    "    # for each group, shuffle and assign it to a set run A1, A2, A3, A4, B1, B2, B3, B4, etc.\n",
    "    set_runs_list = []\n",
    "    # loop through the groups four times, each time shuffle the order and then append taht list to the set runs\n",
    "    for name, group in grouped:\n",
    "        for i in range(4):\n",
    "            # take the id and shuffle the order (but not the contents, just the order) and append to the set runs list\n",
    "            set_runs_list.append(group['id'].sample(frac=1).tolist())\n",
    "    # make a new column in the df and assign the set runs list to it\n",
    "    df['set_run'] = set_runs_list\n",
    "    return df\n",
    "\n",
    "#save the cat_full_list to a csv\n",
    "cat_full_list.to_csv('cat_full_list.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d7d439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tester chunk\n",
    "print(type(cat_full_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88fc5934",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_js_stimuli(df):\n",
    "    js_stimuli = []\n",
    "    for idx, row in df.iterrows():\n",
    "        if idx == 0:  \n",
    "            continue\n",
    "        js_entry = {\n",
    "                \"id\": row['id'],\n",
    "                \"type\": row['length'],\n",
    "                \"anagram\": row['shuffled'],\n",
    "                \"correct\": row['root'],\n",
    "                \"valid\": row['valid_words'],\n",
    "                \"set\": row['Set'],\n",
    "                \"set_run\": row['Set']\n",
    "            }\n",
    "        js_stimuli.append(js_entry) \n",
    "    return js_stimuli\n",
    "\n",
    "js_stimuli = format_js_stimuli(cat_full_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db8fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this chunk should be the thing that saves out the stimuli file in a js. \n",
    "stimuli_js_content = \"let trial_objects = \" + str(all_entries) + \";\"\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(stimuli_js_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae0d590",
   "metadata": {},
   "source": [
    "#  ________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2909e2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor the below functions\n",
    "# create stimulus JSON file\n",
    "\n",
    "def format_js_stimuli(df):\n",
    "    js_stimuli = []\n",
    "    for idx, row in df.iterrows():\n",
    "        if idx == 0:  \n",
    "            continue\n",
    "        js_entry = {\n",
    "                \"id\": row['id'],\n",
    "                \"type\": row['length'],\n",
    "                \"anagram\": row['shuffled'],\n",
    "                \"correct\": row['root'],\n",
    "                \"valid\": row['valid_words'],\n",
    "                \"set\": row['Set'],\n",
    "                \"set_run\": row['Set']\n",
    "            }\n",
    "        js_stimuli.append(js_entry) \n",
    "    return js_stimuli\n",
    "\n",
    "js_stimuli = format_js_stimuli(cat_full_list)\n",
    "# needs refactoring\n",
    "\n",
    "def set_shuffle(word_list, set_name):\n",
    "    shuffled_list = word_list.sample(frac=1).reset_index(drop=True)  # Shuffle the DataFrame\n",
    "    return [\n",
    "        {\n",
    "            \"id\": word[\"id\"],\n",
    "            \"type\": word[\"length\"],\n",
    "            \"anagram\": word[\"shuffled\"],\n",
    "            \"correct\": word[\"root\"],\n",
    "            \"valid\": word[\"valid_words\"],\n",
    "            \"set\": word[\"Set\"],\n",
    "            \"set_run\": set_run,\n",
    "        }\n",
    "        for _, word in shuffled_list.iterrows()\n",
    "    ]\n",
    "\n",
    "def run_set_shuffle(df): # function to shuffle words in each set and assign to a unique run sequence called a set_run\n",
    "    all_entries = []\n",
    "    if isinstance(df, list):\n",
    "        df = pd.DataFrame(df)\n",
    "    for run_number in range(1, 5):  # Create 4 runs for each set\n",
    "        for set_name in ['SetA', 'SetB', 'SetC', 'SetD']:\n",
    "            set_run = f\"{set_name}{run_number}\" \n",
    "            set_df = df[df['SetRun'] == set_run] # filter the df by the set run\n",
    "            shuffled_set = set_shuffle(set_df, set_name)\n",
    "            all_entries.extend(shuffled_set)\n",
    "    return all_entries\n",
    "\n",
    "\n",
    "print(type(js_stimuli))\n",
    "js_stimuli = run_set_shuffle(js_stimuli)\n",
    "print(js_stimuli)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41114717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refactor the below functions to work with new frame\n",
    "\n",
    "# Placeholder for all JS entries\n",
    "all_entries = []\n",
    "\n",
    "# Loop over each CSV file and set name\n",
    "for i in range(4):\n",
    "    filename = f\"group_{i+1}_word_pairs.csv\"\n",
    "    set_name = f\"Set{chr(65 + i)}\"  # 'SetA', 'SetB', 'SetC', 'SetD'\n",
    "\n",
    "    with open(filename, newline=\"\") as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        js_entries = csv_to_js_format(\n",
    "            csvreader, set_name\n",
    "        )  # Pass csvreader and set_name\n",
    "        all_entries.extend(js_entries)\n",
    "\n",
    "## Writing the JS file\n",
    "# Save all entries into the JS file\n",
    "stimuli_js_content = \"let trial_objects = \" + str(all_entries) + \";\"\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(stimuli_js_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa26ca32",
   "metadata": {},
   "source": [
    "# Depreciated \n",
    "## Now we start making the stimulus files\n",
    "\n",
    "First we need to distribute the unique words among the number of groups. Groups then turn into CSV files. \n",
    "We also need to make the json file that's used for the stimulus in the experiment but I prefer it to be formated in a way that's legible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a7fe5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ----- Depreciated Functions ----- ###\n",
    "\n",
    "def wordlist2length_dict(anagram_list):\n",
    "    #  grouped by the lengths of the words\n",
    "    anagram_by_length = {}\n",
    "    for word in anagram_list:\n",
    "        length = len(word)\n",
    "        if length not in anagram_by_length:\n",
    "            anagram_by_length[length] = []\n",
    "        anagram_by_length[length].append(word)\n",
    "    return anagram_by_length\n",
    "        \n",
    "def distribute_words(anagram_by_length, num_set = 4):\n",
    "    #  distribute the words to the sets\n",
    "    anagram_sets = {}\n",
    "    for length, words in anagram_by_length.items():\n",
    "        random.shuffle(words)\n",
    "        anagram_sets[length] = [words[i::num_set] for i in range(num_set)]\n",
    "    return anagram_sets\n",
    "\n",
    "\n",
    "# function to join the sets into unique collumns named group_a-d based off of the rows of the df\n",
    "def join_sets_into_groups(anagram_sets, list):\n",
    "    test_group = pd.DataFrame()\n",
    "    test_list_length = len(list)\n",
    "    for i in range(test_list_length):\n",
    "        group = pd.DataFrame()\n",
    "        for length, words in anagram_sets.items():\n",
    "            group[f'length_{length}'] = words[i]\n",
    "        test_group = pd.concat([test_group, group], axis=1)\n",
    "    return test_group.transpose()\n",
    "\n",
    "# each row of a set is joined into a new list, joined into a new df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b459fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### testing \n",
    "######\n",
    "# create a test list with 20 words of length 4, 5, and 6\n",
    "import random\n",
    "import string\n",
    "\n",
    "def generate_random_word(length):\n",
    "    return ''.join(random.choices(string.ascii_lowercase, k=length))\n",
    "\n",
    "# Generate a list of 20 words with lengths 4, 5, and 6\n",
    "word_lengths = [4, 5, 6]\n",
    "test_list = [generate_random_word(length) for _ in range(20) for length in word_lengths]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48814bc8",
   "metadata": {},
   "source": [
    "## New Functions\n",
    "-- distribute into groups\n",
    "-- make csv \n",
    "-- make csv turn into the json file \n",
    "-- shuffle the run order and assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f94913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a distributing function to spread the words across the four groups\n",
    "# def distribute_words(word_list, num_sets):\n",
    "#     # Calculate the number of words per group\n",
    "#     group_size = len(word_list) // num_sets\n",
    "\n",
    "#     # Create groups\n",
    "#     groups = [[] for _ in range(num_sets)]\n",
    "\n",
    "#     # Distribute words to each group\n",
    "#     for index, word in enumerate(word_list):\n",
    "#         group_index = index // group_size\n",
    "#         if (\n",
    "#             group_index < num_sets\n",
    "#         ):  # This check prevents index out of range if not perfectly divisible\n",
    "#             groups[group_index].append(word)\n",
    "\n",
    "#     return groups\n",
    "\n",
    "# need to mod this to take an array and convert to js format\n",
    "def format_js_stimuli(sets):\n",
    "    js_stimuli = []\n",
    "    for idx, row in enumerate(sets):\n",
    "        if idx == 0:  \n",
    "            continue\n",
    "        word_type, word_pair = row\n",
    "        original, anagram = word_pair.split(\", \")\n",
    "        js_entry = {\n",
    "            \"id\": f\"{str(idx + 1).zfill(3)}\",  # Assign unique IDs starting from 001\n",
    "            \"type\": word_type,\n",
    "            \"anagram\": anagram,\n",
    "            \"correct\": original,\n",
    "            \"set\": set_name,\n",
    "        }\n",
    "        js_entries.append(js_entry) \n",
    "    return js_stimuli\n",
    "\n",
    "\n",
    "def csv_to_js_format(csv_content, set_name):\n",
    "    js_entries = []\n",
    "    for idx, row in enumerate(csv_content):\n",
    "        if idx == 0:  # Skip header row\n",
    "            continue\n",
    "        word_type, word_pair = row\n",
    "        original, anagram = word_pair.split(\", \")\n",
    "        js_entry = {\n",
    "            \"id\": f\"{str(idx + 1).zfill(3)}\",  # Assign unique IDs starting from 001\n",
    "            \"type\": word_type,\n",
    "            \"anagram\": anagram,\n",
    "            \"correct\": original,\n",
    "            \"set\": set_name,\n",
    "        }\n",
    "        js_entries.append(js_entry)\n",
    "    return js_entries\n",
    "\n",
    "# Function to shuffle a set of words and include set run order\n",
    "def set_shuffle(word_list, set_name, run_number):\n",
    "    shuffled_list = word_list[:]  # Create a copy of the word_list to shuffle\n",
    "    random.shuffle(shuffled_list)  # Shuffle the list of words\n",
    "    set_run = f\"{set_name}{run_number}\"\n",
    "    return [\n",
    "        {\n",
    "            \"id\": word[\"id\"],\n",
    "            \"type\": word[\"type\"],\n",
    "            \"anagram\": word[\"anagram\"],\n",
    "            \"correct\": word[\"correct\"],\n",
    "            \"set\": word[\"set\"],\n",
    "            \"setRun\": set_run,\n",
    "        }\n",
    "        for word in shuffled_list\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e541ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the possible_words_dict into a json file in which the key is the shuffled word and the value is the list of possible words\n",
    "# all_possible_words_json = json.dumps(possible_words_dict)\n",
    "# with open(\"possible_words.json\", \"w\") as file:\n",
    "#     file.write(all_possible_words_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf7d84",
   "metadata": {},
   "source": [
    "<h3> Concatinate and create the Sets </h3>\n",
    "\n",
    "So now we need to make the stimuli for each of the blocks. For this we want to have 10 strings of each length per block for 3 blocks. \n",
    "\n",
    "And then we make groups of those until all the words are assigned. Finally, we will collect 10-30 participants per group. \n",
    "\n",
    "So in total: 3 blocks of 30 anagrams which will be 10 four letters, 10 five letters, 10 six letters. To make sure we get all the words, we will need 4 groups to have the 120 words for each string length represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3f61ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_groups = 4\n",
    "# making a distributing function to spread the words across the four groups\n",
    "def distribute_words(word_list, num_groups):\n",
    "    # Calculate the number of words per group\n",
    "    group_size = len(word_list) // num_groups\n",
    "\n",
    "    # Create groups\n",
    "    groups = [[] for _ in range(num_groups)]\n",
    "\n",
    "    # Distribute words to each group\n",
    "    for index, word in enumerate(word_list):\n",
    "        group_index = index // group_size\n",
    "        if (\n",
    "            group_index < num_groups\n",
    "        ):  # This check prevents index out of range if not perfectly divisible\n",
    "            groups[group_index].append(word)\n",
    "\n",
    "    return groups\n",
    "\n",
    "# Save each group to a separate CSV file-- we do this so we can review the words in each group and how they are distributed.\n",
    "# for i in range(4):\n",
    "#     filename = f\"group_{i+1}_word_pairs.csv\"\n",
    "#     with open(filename, \"w\", newline=\"\") as file:\n",
    "#         writer = csv.writer(file)\n",
    "#         writer.writerow([\"Type\", \"Word Pairs\"])\n",
    "#         writer.writerows(\n",
    "#             [[\"Four-Letter\", word] for word in grouped_four_letter_words[i]]\n",
    "#         )\n",
    "#         writer.writerows(\n",
    "#             [[\"Five-Letter\", word] for word in grouped_five_letter_words[i]]\n",
    "#         )\n",
    "#         writer.writerows([[\"Six-Letter\", word] for word in grouped_six_letter_words[i]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7555b9ca",
   "metadata": {},
   "source": [
    "Okay now we need to create the stimuli file which should be in a .js that will look like: \n",
    "\n",
    "let trial_objects = [\n",
    "    {\n",
    "        \"id\": \"001\",\n",
    "        \"type\": \"Four-Letter\",\n",
    "        \"anagram\": \"atth\",\n",
    "        \"correct\": \"that\",\n",
    "        \"set\": \"A\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bba816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv_directory = (\n",
    "#     \"./group_1_word_pairs.csv\",\n",
    "#     \"./group_2_word_pairs.csv\",\n",
    "#     \"./group_3_word_pairs.csv\",\n",
    "#     \"./group_4_word_pairs.csv\",\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# Placeholder for all JS entries\n",
    "all_entries = []\n",
    "\n",
    "# Loop over each CSV file and set name\n",
    "for i in range(4):\n",
    "    filename = f\"group_{i+1}_word_pairs.csv\"\n",
    "    set_name = f\"Set{chr(65 + i)}\"  # 'SetA', 'SetB', 'SetC', 'SetD'\n",
    "\n",
    "    with open(filename, newline=\"\") as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        js_entries = csv_to_js_format(\n",
    "            csvreader, set_name\n",
    "        )  # Pass csvreader and set_name\n",
    "        all_entries.extend(js_entries)\n",
    "\n",
    "## Writing the JS file\n",
    "# Save all entries into the JS file\n",
    "stimuli_js_content = \"let trial_objects = \" + str(all_entries) + \";\"\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(stimuli_js_content)\n",
    "\n",
    "# modfiying the stimuli.js file to be in the preferred format (mostly for readability)\n",
    "# Here we format the JSON with specific spacing and bracketing style\n",
    "stimuli_js_content = \"let trial_objects = [\\n\"\n",
    "for entry in all_entries:\n",
    "    stimuli_js_content += \"    \" + json.dumps(entry, indent=4) + \",\\n\"\n",
    "stimuli_js_content = stimuli_js_content.rstrip(\",\\n\") + \"\\n];\"\n",
    "\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(stimuli_js_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a2c41",
   "metadata": {},
   "source": [
    "Alright we've now got a set but we are gonna take each set and then shuffle into 4 coded runs.\n",
    "Essentially, by trying to randomize the order on the fly, we can introduce a bunch of bugs into the timeline variables. \n",
    "By hardcoding 4 unique run orders for each set of words, we are addressing order effects without risking more bugs. \n",
    "\n",
    "So below we take the stimuli file, filter by set, add a new parameter to the stimuli file which is its run order assignment which is numbered 1-4. \n",
    "Now ALL of the words in set A will be shuffled into 4 unique orders and assigned to A1, A2, A3, A4. All the A words are the same but their order is now randomized.\n",
    "The this will be saved into the stimuli file as \"SetRun\":\"A1\" etc etc.\n",
    "\n",
    "Now this will make the Stimuli.js file increase substancially (by four) but will keep our code clean and modular. Also fewer headaches since the other option is adjusting the JS utility file that makes the variable order and we really don't wanna do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afebca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Placeholder for all JS entries\n",
    "all_entries = []\n",
    "\n",
    "# Loop over each CSV file and set name\n",
    "for i in range(4):\n",
    "    filename = f\"group_{i+1}_word_pairs.csv\"\n",
    "    set_name = f\"Set{chr(65 + i)}\"  # 'SetA', 'SetB', 'SetC', 'SetD'\n",
    "\n",
    "    with open(filename, newline=\"\") as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        csv_content = list(csvreader)  # Convert csvreader to a list\n",
    "        js_entries = csv_to_js_format(\n",
    "            csv_content, set_name\n",
    "        )  # Pass csv_content and set_name\n",
    "        for run_number in range(1, 5):  # Create 4 runs for each set\n",
    "            shuffled_set = set_shuffle(js_entries, set_name, run_number)\n",
    "            all_entries.extend(shuffled_set)\n",
    "\n",
    "# Save all entries into the JS file\n",
    "stimuli_js_content = \"let trial_objects = \" + json.dumps(all_entries, indent=4) + \";\"\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(stimuli_js_content)\n",
    "\n",
    "# Count occurrences of each id... okay well this isn't right but also not really important, so I'll leave it as is. What's happening is that the id is being counted but the assignment is made during a loop and so for each id there is 4 unique string pairs. for example 032 is about, water, birth, and trust.\n",
    "id_counter = Counter(entry[\"id\"] for entry in all_entries)\n",
    "print(\"ID Occurrences:\")\n",
    "for id_, count in id_counter.items():\n",
    "    print(f\"id {id_}: {count} times\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caac0b83",
   "metadata": {},
   "source": [
    "Okay I'm just looking for a sanity check and gonna run a loop over the stimuli js file to correct the  ID and then see if that does it. I think this notebook should be a code review for lab someday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0785e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the stimuli file\n",
    "with open(\"stimuli.js\", \"r\") as file:\n",
    "    stimuli_js_content = file.read()\n",
    "\n",
    "# Extract the JSON data from the stimuli file\n",
    "json_data = json.loads(stimuli_js_content[len(\"let trial_objects = \") : -1])\n",
    "\n",
    "# Create a mapping for unique anagrams to new IDs\n",
    "unique_anagram_to_id = {}\n",
    "id_counter = 1\n",
    "\n",
    "for entry in json_data:\n",
    "    anagram = entry[\"anagram\"]\n",
    "    if anagram not in unique_anagram_to_id:\n",
    "        unique_anagram_to_id[anagram] = f\"{id_counter:03d}\"\n",
    "        id_counter += 1\n",
    "\n",
    "# Reassign IDs in the JSON data\n",
    "for entry in json_data:\n",
    "    entry[\"id\"] = unique_anagram_to_id[entry[\"anagram\"]]\n",
    "\n",
    "# Save the updated entries into the JS file\n",
    "updated_stimuli_js_content = (\n",
    "    \"let trial_objects = \" + json.dumps(json_data, indent=4) + \";\"\n",
    ")\n",
    "with open(\"updated_stimuli.js\", \"w\") as file:\n",
    "    file.write(updated_stimuli_js_content)\n",
    "\n",
    "# Print the new ID mapping for verification\n",
    "print(\"New ID mapping for unique anagrams:\")\n",
    "for anagram, new_id in unique_anagram_to_id.items():\n",
    "    print(f\"Anagram: {anagram}, New ID: {new_id}\")\n",
    "\n",
    "# This worked okay lets save to the stimuli.js file\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(updated_stimuli_js_content)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python (miniconda3)",
   "language": "python",
   "name": "miniconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
