{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cca7df1d",
   "metadata": {},
   "source": [
    "<h2> Generating the Stimuli file</h2>\n",
    "    \n",
    "This notebook is a modified version of prior work to generate the stimuli files used in anagram experiments.\n",
    "\n",
    "First we grab a list of words and then shuffle. From the shuffled set we then set up seperate groups (set groups) from there we use those paired shuffle and \"correct\" words to make the stimuli file used in the anagram experiments. \n",
    "\n",
    "This one will differ since I'll be including the sources and frequency data on use of the words in the set. Furthermore, I will be using this notebook to also make a json object of valid solutions for each shuffle. The stimuli file we use includes the word used for shuffling however, those shuffled strings can be solved to more than one real english word which is how we define \"valid\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ce3545",
   "metadata": {},
   "source": [
    "We can use dictionaries that have all the words of specific strings than use a function to give us some number of those words. \n",
    "\n",
    "#### The word bank we are using is from Word Net:\n",
    "    George A. Miller (1995). WordNet: A Lexical Database for English.\n",
    "    Communications of the ACM Vol. 38, No. 11: 39-41.\n",
    "    Christiane Fellbaum (1998, ed.) WordNet: An Electronic Lexical Database. Cambridge, MA: MIT Press.\n",
    "    WordNet: An Electronic Lexical Database\n",
    "\n",
    "#### The frequency information is from the word freq: \n",
    "    Robyn Speer. (2022). rspeer/wordfreq: v3.0 (v3.0.2). Zenodo. https://doi.org/10.5281/zenodo.7199437\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c67cb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lyndefolsom/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nltk \n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import ngrams, FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "import wordfreq\n",
    "from wordfreq import zipf_frequency\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387d166f",
   "metadata": {},
   "source": [
    "<h3> Five letter words </h3>\n",
    "\n",
    "Starting with 5 letters. We make a dictionary that's got the synset info (lemma is the word) and then we grab the information about the frequency of that word to sort a list. Finally we will make a lil function that grabs our words to make a set. \n",
    "\n",
    "After we will need to qc the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee42a10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'there', 'other', 'first', 'after', 'think', 'right', 'being', 'going', 'still', 'never', 'world', 'great', 'no-go', 'while', 'every', 'three', 'state', 'under', 'thing', 'house', 'place', 'again', 'found', 'might', 'money', 'night', 'group', 'start', 'times', 'today', 'point', 'music', 'power', 'water', 'white', 'small', 'based', 'later', 'order', 'party', 'thank', 'black', 'whole', 'story', 'least', 'means', 'early', 'local', 'young', 'video', 'given', 'level', 'often', 'court', 'south', 'death', 'hours', 'large', 'wrong', 'known', 'along', 'close', 'class', 'happy', 'human', 'cause', 'woman', 'north', 'watch', 'leave', 'd-day', 'taken', 'light', 'short', 'third', 'check', 'heart', 'major', 'phone', 'child', 'quite', 'works', 'ready', 'front', 'final', 't-man', 'heard', 'bring', 'march', 'study', 'clear', 'words', 'month', 'field', 'board', 'space', 'fight', 'issue', 'force', 'price', 'total', 'above', 'share', 'sense', 'april', 'event', 'break', 'guess', 'learn', 'alone', 'worth', 'press', 'hands', 'movie', 'sound', 'value', 'round', 'stand', 'stuff', 'green', 'drive', 'model', 'match', 'trust', 'david', 'trade', 'range', 'chief', 'style', 'james', 'lower', 'stage', 'china', 'blood', 'title', 'enjoy', 'super', 'legal', 'union', 'seven', 'cover', 'staff', 'crazy', 'built', 'daily', 'voice', 'paper', 'parts', 'earth', 'below', 'table', 'truth', 'offer', 'sleep', 'visit', 'piece', 'india', 'build', 'river', 'speak', 'write', 'eight', 'funny', 'track', 'album', 'peace', 'store', 'brown', 'moved', 'ahead', 'radio', 'v-day', 'allow', 'cross', 'loved', 'speed', 'focus', 'jesus', 'extra', 'quick', 'clean', 'scene', 'agree', 'spend', 'heavy', 'coach', 'train', 'costs', 'claim', 'hotel', 'judge', 'reach', 'floor', 'image', 'brain', 'g-man', 'worst', 'civil', 'stock']\n"
     ]
    }
   ],
   "source": [
    "# make a dictionary of five string words from wordnet\n",
    "fivestrings = {}\n",
    "for synset in wn.all_synsets():\n",
    "    if len(synset.name().split('.')) == 3:\n",
    "         if len(synset.name().split('.')[0]) == 5:\n",
    "               fivestrings[synset.name().split('.')[0]] = synset.definition()\n",
    "# need to sort dictionary by word frequency in the english language so that the most common words are first\n",
    "# we use the wordfreq library and the zipf_frequency function to get the frequency of each word in the dictionary and then sort the dictionary by frequency\n",
    "five_frequencies = {}\n",
    "for word in fivestrings:\n",
    "    five_frequencies[word] = zipf_frequency(word, 'en')\n",
    "sorted_fdist = sorted(five_frequencies, key=five_frequencies.get, reverse=True)\n",
    "\n",
    "# now we get the list and make a function to give us some number of words in that list based on the frequency\n",
    "def get_words(n):\n",
    "    return sorted_fdist[:n]\n",
    "\n",
    "#### Now we just define the length of our list we want to get and call the function to get the list\n",
    "five_letter_words = get_words(200)\n",
    "print(five_letter_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f427eb",
   "metadata": {},
   "source": [
    "Shuffle time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02aba229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will qc the set of words and remove any that are not 5 letters long. This includes removing any string that has a hyphen in it or is a name like \"jesus\"\n",
    "five_letter_words = [word for word in five_letter_words if len(word) == 5 and \"-\" not in word and word.isalpha()]\n",
    "\n",
    "# Shuffle each word's letters and pair them together ** adjust to the length of list wanted. \n",
    "shuffled_pairs_five = []\n",
    "shuffled_five = []\n",
    "for word in five_letter_words:\n",
    "    shuffled = list(word)\n",
    "    random.shuffle(shuffled)\n",
    "    shuffled_word = ''.join(shuffled)\n",
    "    shuffled_pairs_five.append(f\"{word}, {shuffled_word}\")\n",
    "    shuffled_five.append(f\"{shuffled_word}\")\n",
    "    #set to length desired\n",
    "shuffled_pairs_five =  shuffled_pairs_five[:180]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554a050",
   "metadata": {},
   "source": [
    "<h3> Six letter words and shuffled </h3>\n",
    "\n",
    "Six letters will follow the strategy from five letters where we make a 6 letter dictionary and then sort by the frequency and then qc and shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aba2f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary of five string words from wordnet\n",
    "sixstrings = {}\n",
    "for synset in wn.all_synsets():\n",
    "    if len(synset.name().split('.')) == 3:\n",
    "         if len(synset.name().split('.')[0]) == 6:\n",
    "               sixstrings[synset.name().split('.')[0]] = synset.definition()\n",
    "# need to sort dictionary by word frequency in the english language so that the most common words are first\n",
    "# we use the wordfreq library and the zipf_frequency function to get the frequency of each word in the dictionary and then sort the dictionary by frequency\n",
    "six_frequencies = {}\n",
    "for word in sixstrings:\n",
    "    six_frequencies[word] = zipf_frequency(word, 'en')\n",
    "sorted_fdist = sorted(six_frequencies, key=six_frequencies.get, reverse=True)\n",
    "\n",
    "# now we get the list and make a function to give us some number of words in that list based on the frequency\n",
    "def get_words(n):\n",
    "    return sorted_fdist[:n]\n",
    "\n",
    "#### Now we just define the length of our list we want to get and call the function to get the list\n",
    "six_letter_words = get_words(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91abea7c",
   "metadata": {},
   "source": [
    "Shuffle time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a39620e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will qc the set of words and remove any that are not 5 letters long. This includes removing any string that has a hyphen in it or is a name like \"jesus\"\n",
    "six_letter_words = [word for word in six_letter_words if len(word) == 6 and \"-\" not in word and word.isalpha()]\n",
    "\n",
    "# Shuffle each word's letters and pair them together\n",
    "shuffled_six_pairs = []\n",
    "shuffled_six = []\n",
    "for word in six_letter_words:\n",
    "    shuffled = list(word)\n",
    "    random.shuffle(shuffled)\n",
    "    shuffled_word = ''.join(shuffled)\n",
    "    shuffled_six_pairs.append(f\"{word}, {shuffled_word}\")\n",
    "    shuffled_six.append(f\"{shuffled_word}\")\n",
    "    # set the count to desired length\n",
    "    shuffled_six_pairs= shuffled_six_pairs[:180]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d1fe59",
   "metadata": {},
   "source": [
    "<h3> Finaly the four letter words </h3>\n",
    "\n",
    "There may be additional qc lines needed but lets see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aa9cb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['have', 'will', 'just', 'like', 'more', 'time', 'some', 'good', 'only', 'know', 'over', 'make', 'then', 'back', 'want', 'well', 'much', 'most', 'very', 'even', 'here', 'need', 'work', 'year', 'made', 'take', 'many', 'life', 'down', 'last', 'best', 'such', 'love', 'long', 'home', 'look', 'same', 'used', 'both', 'part', 'come', 'find', 'high', 'help', 'game', 'give', 'next', 'each', 'must', 'show', 'sure', 'feel', 'team', 'ever', 'keep', 'free', 'away', 'left', 'play', 'name', 'city', 'days', 'real', 'done', 'care', 'week', 'case', 'live', 'full', 'read', 'hard', 'mean', 'four', 'once', 'tell', 'stop', 'call', 'head', 'side', 'less', 'line', 'open', 'shit', 'area', 'kind', 'five', 'face', 'hope', 'news', 'able', 'post', 'book', 'talk', 'half', 'hand', 'mind', 'fact', 'true', 'food', 'body', 'lost', 'fuck', 'room', 'john', 'girl', 'nice', 'york', 'past', 'idea', 'move', 'wait', 'late', 'data', 'stay', 'soon', 'turn', 'deal', 'form', 'fire', 'easy', 'near', 'west', 'plan', 'list', 'type', 'meet', 'song', 'baby', 'word', 'self', 'main', 'held', 'road', 'town', 'cost', 'fine', 'rest', 'term', 'wife', 'hear', 'shot', 'miss', 'date', 'site', 'land', 'eyes', 'june', 'lead', 'club', 'film', 'dead', 'test', 'view', 'hold', 'star', 'hour', 'wish', 'gone', 'gold', 'king', 'july', 'east', 'sent', 'bank', 'role', 'park', 'cool', 'bill', 'a-ok', 'save', 'rate', 'fast', 'blue', 'size', 'step', 'fall', 'felt', 'paid', 'page', 'vote', 'hate', 'send', 'hell', 'lord', 'damn', 'poor', 'born', 'kill', 'walk', 'pick', 'hair', 'door', 'code', 'sign', 'race', 'lose', 'seem', 'safe', 'loss', 'huge']\n"
     ]
    }
   ],
   "source": [
    "# make a dictionary of four string words from wordnet\n",
    "fourstrings = {}\n",
    "for synset in wn.all_synsets():\n",
    "    if len(synset.name().split('.')) == 3:\n",
    "         if len(synset.name().split('.')[0]) == 4:\n",
    "               fourstrings[synset.name().split('.')[0]] = synset.definition()\n",
    "# need to sort dictionary by word frequency in the english language so that the most common words are first\n",
    "# we use the wordfreq library and the zipf_frequency function to get the frequency of each word in the dictionary and then sort the dictionary by frequency\n",
    "four_frequencies = {}\n",
    "for word in fourstrings:\n",
    "    four_frequencies[word] = zipf_frequency(word, 'en')\n",
    "sorted_fdist = sorted(four_frequencies, key=four_frequencies.get, reverse=True)\n",
    "\n",
    "# now we get the list and make a function to give us some number of words in that list based on the frequency\n",
    "def get_words(n):\n",
    "    return sorted_fdist[:n]\n",
    "\n",
    "#### Now we just define the length of our list we want to get and call the function to get the list\n",
    "four_letter_words = get_words(200)\n",
    "print(four_letter_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a10ca2",
   "metadata": {},
   "source": [
    "Shuffle time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83d1d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like we don't need to qc but lets also make sure there isn't hyphens or words that are shorter or longer than four letters\n",
    "four_letter_words = [word for word in four_letter_words if len(word) == 4 and \"-\" not in word and word.isalpha()]\n",
    "\n",
    "# suffle\n",
    "shuffled_four_pairs = []\n",
    "shuffled_fours = []\n",
    "for word in four_letter_words:\n",
    "    shuffled = list(word)\n",
    "    while True:\n",
    "        random.shuffle(shuffled)\n",
    "        shuffled_word = ''.join(shuffled)\n",
    "        if shuffled_word != word:\n",
    "            break\n",
    "    shuffled_four_pairs.append(f\"{word}, {shuffled_word}\")\n",
    "    shuffled_fours.append(f\"{shuffled_word}\")\n",
    "    \n",
    "shuffled_four_pairs = shuffled_four_pairs[:180]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882fe757",
   "metadata": {},
   "source": [
    "## Make a solution key\n",
    "\n",
    "Many anagrams have more than one response that is a valid solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd284b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the the fivestring, sixstring, and fourstring dictionaries we will make a function to check if a word is in the dictionary and return the definition of the word if it is.\n",
    "def check_word(word):\n",
    "    if word in fivestrings:\n",
    "        return f\"{word}: {fivestrings[word]}\"\n",
    "    elif word in sixstrings:\n",
    "        return f\"{word}: {sixstrings[word]}\"\n",
    "    elif word in fourstrings:\n",
    "        return f\"{word}: {fourstrings[word]}\"\n",
    "    else:\n",
    "        return f\"{word} is not in the dictionary\"\n",
    "# that function we can embed in a loop that checks if a permutation is in our dictionary.\n",
    "# we will need to first keep each string list separate to then reorder the letters and then check if that is a word, if so we we add it, otherwise we move to next word\n",
    "# we will make a permutation such that \"baout\" will be arranged to \"outba\" check if that's a word, if not move to next permutation\n",
    "import itertools\n",
    "\n",
    "def find_valid_words(input_string):\n",
    "    valid_words = []\n",
    "    permutations = itertools.permutations(input_string)\n",
    "    \n",
    "    for perm in permutations:\n",
    "        perm_word = ''.join(perm)\n",
    "        result = check_word(perm_word)\n",
    "        if \"is not in the dictionary\" not in result:\n",
    "            valid_words.append(perm_word)\n",
    "    \n",
    "    return valid_words\n",
    "\n",
    "# now we go through the list of shuffled words use the find_valid_words for each word and append the results to a list \n",
    "five_possible_words = []\n",
    "for word in shuffled_five:\n",
    "    possible_words = find_valid_words(word)\n",
    "    five_possible_words.append(possible_words)\n",
    "six_possible_words = []\n",
    "for word in shuffled_six:\n",
    "    possible_words = find_valid_words(word)\n",
    "    six_possible_words.append(possible_words)\n",
    "four_possible_words = []\n",
    "for word in shuffled_fours:\n",
    "    possible_words = find_valid_words(word)\n",
    "    four_possible_words.append(possible_words)\n",
    "\n",
    "# aggregate all of these lists into one list\n",
    "all_possible_words = five_possible_words + six_possible_words + four_possible_words\n",
    "# convert the list into a dictionary with the shuffled word as the key and the list of possible words as the value\n",
    "possible_words_dict = dict(zip(shuffled_five + shuffled_six + shuffled_fours, all_possible_words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa26ca32",
   "metadata": {},
   "source": [
    "## With an All Possible file we can check valid\n",
    "Now that we have all the possible words that can be made from a particular string, we can use that dictionary as a json that could be read into the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e541ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the possible_words_dict into a json file in which the key is the shuffled word and the value is the list of possible words\n",
    "all_possible_words_json = json.dumps(possible_words_dict)\n",
    "with open(\"possible_words.json\", \"w\") as file:\n",
    "    file.write(all_possible_words_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf7d84",
   "metadata": {},
   "source": [
    "<h3> Concatinate and create the Sets </h3>\n",
    "\n",
    "So now we need to make the stimuli for each of the blocks. For this we want to have 10 strings of each length per block for 3 blocks. \n",
    "\n",
    "And then we make groups of those until all the words are assigned. Finally, we will collect 10-30 participants per group. \n",
    "\n",
    "So in total: 3 blocks of 30 anagrams which will be 10 four letters, 10 five letters, 10 six letters. To make sure we get all the words, we will need 4 groups to have the 120 words for each string length represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3f61ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a distributing function to spread the words across the four groups\n",
    "def distribute_words(word_list, num_groups=4):\n",
    "    # Calculate the number of words per group\n",
    "    group_size = len(word_list) // num_groups\n",
    "    \n",
    "    # Create groups\n",
    "    groups = [[] for _ in range(num_groups)]\n",
    "    \n",
    "    # Distribute words to each group\n",
    "    for index, word in enumerate(word_list):\n",
    "        group_index = index // group_size\n",
    "        if group_index < num_groups:  # This check prevents index out of range if not perfectly divisible\n",
    "            groups[group_index].append(word)\n",
    "    \n",
    "    return groups\n",
    "\n",
    "# Distribute words to the groups\n",
    "grouped_four_letter_words = distribute_words(shuffled_four_pairs)\n",
    "grouped_five_letter_words = distribute_words(shuffled_pairs_five)\n",
    "grouped_six_letter_words = distribute_words(shuffled_six_pairs)\n",
    "\n",
    "# Save each group to a separate CSV file-- we do this so we can review the words in each group and how they are distributed. \n",
    "for i in range(4):\n",
    "    filename = f'group_{i+1}_word_pairs.csv'\n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Type', 'Word Pairs'])\n",
    "        writer.writerows([['Four-Letter', word] for word in grouped_four_letter_words[i]])\n",
    "        writer.writerows([['Five-Letter', word] for word in grouped_five_letter_words[i]])\n",
    "        writer.writerows([['Six-Letter', word] for word in grouped_six_letter_words[i]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7555b9ca",
   "metadata": {},
   "source": [
    "Okay now we need to create the stimuli file which should be in a .js that will look like: \n",
    "\n",
    "let trial_objects = [\n",
    "    {\n",
    "        \"id\": \"001\",\n",
    "        \"type\": \"Four-Letter\",\n",
    "        \"anagram\": \"atth\",\n",
    "        \"correct\": \"that\",\n",
    "        \"set\": \"A\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bba816",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_directory = \"./group_1_word_pairs.csv\", \"./group_2_word_pairs.csv\", \"./group_3_word_pairs.csv\", \"./group_4_word_pairs.csv\"\n",
    "\n",
    "# Function to convert CSV content to JS format\n",
    "def csv_to_js_format(csv_content, set_name):\n",
    "    js_entries = []\n",
    "    for idx, row in enumerate(csv_content):\n",
    "        word_type, word_pair = row\n",
    "        if idx == 0:  # Skip header row\n",
    "            continue\n",
    "        original, anagram = word_pair.split(\", \")\n",
    "        js_entry = {\n",
    "            \"id\": f\"{str(idx).zfill(3)}\",\n",
    "            \"type\": word_type,\n",
    "            \"anagram\": anagram,\n",
    "            \"correct\": original,\n",
    "            \"set\": set_name\n",
    "        }\n",
    "        js_entries.append(js_entry)\n",
    "    return js_entries\n",
    "\n",
    "# Placeholder for all JS entries\n",
    "all_entries = []\n",
    "\n",
    "# Loop over each CSV file and set name\n",
    "for i in range(4):\n",
    "    filename = f'group_{i+1}_word_pairs.csv'\n",
    "    set_name = f\"Set{chr(65 + i)}\"  # 'SetA', 'SetB', 'SetC', 'SetD'\n",
    "    \n",
    "    with open(filename, newline='') as csvfile:\n",
    "     csvreader = csv.reader(csvfile)\n",
    "     js_entries = csv_to_js_format(csvreader, set_name)  # Pass csvreader and set_name\n",
    "     all_entries.extend(js_entries)\n",
    "\n",
    "## Writing the JS file\n",
    "# Save all entries into the JS file\n",
    "stimuli_js_content = \"let trial_objects = \" + str(all_entries) + \";\"\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(stimuli_js_content)\n",
    "\n",
    "#modfiying the stimuli.js file to be in the preferred format (mostly for readability)\n",
    "# Here we format the JSON with specific spacing and bracketing style\n",
    "stimuli_js_content = \"let trial_objects = [\\n\"\n",
    "for entry in all_entries:\n",
    "    stimuli_js_content += \"    \" + json.dumps(entry, indent=4) + \",\\n\"\n",
    "stimuli_js_content = stimuli_js_content.rstrip(\",\\n\") + \"\\n];\"\n",
    "\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(stimuli_js_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a2c41",
   "metadata": {},
   "source": [
    "Alright we've now got a set but we are gonna take each set and then shuffle into 4 coded runs.\n",
    "Essentially, by trying to randomize the order on the fly, we can introduce a bunch of bugs into the timeline variables. \n",
    "By hardcoding 4 unique run orders for each set of words, we are addressing order effects without risking more bugs. \n",
    "\n",
    "So below we take the stimuli file, filter by set, add a new parameter to the stimuli file which is its run order assignment which is numbered 1-4. \n",
    "Now ALL of the words in set A will be shuffled into 4 unique orders and assigned to A1, A2, A3, A4. All the A words are the same but their order is now randomized.\n",
    "The this will be saved into the stimuli file as \"SetRun\":\"A1\" etc etc.\n",
    "\n",
    "Now this will make the Stimuli.js file increase substancially (by four) but will keep our code clean and modular. Also fewer headaches since the other option is adjusting the JS utility file that makes the variable order and we really don't wanna do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afebca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert CSV content to JS format\n",
    "def csv_to_js_format(csv_content, set_name):\n",
    "    js_entries = []\n",
    "    for idx, row in enumerate(csv_content):\n",
    "        if idx == 0:  # Skip header row\n",
    "            continue\n",
    "        word_type, word_pair = row\n",
    "        original, anagram = word_pair.split(\", \")\n",
    "        js_entry = {\n",
    "            \"id\": f\"{str(idx + 1).zfill(3)}\",  # Assign unique IDs starting from 001\n",
    "            \"type\": word_type,\n",
    "            \"anagram\": anagram,\n",
    "            \"correct\": original,\n",
    "            \"set\": set_name\n",
    "        }\n",
    "        js_entries.append(js_entry)\n",
    "    return js_entries\n",
    "\n",
    "# Function to shuffle a set of words and include set run order\n",
    "def set_shuffle(word_list, set_name, run_number):\n",
    "    shuffled_list = word_list[:]  # Create a copy of the word_list to shuffle\n",
    "    random.shuffle(shuffled_list)  # Shuffle the list of words\n",
    "    set_run = f\"{set_name}{run_number}\"\n",
    "    return [{\n",
    "        \"id\": word[\"id\"],\n",
    "        \"type\": word[\"type\"],\n",
    "        \"anagram\": word[\"anagram\"],\n",
    "        \"correct\": word[\"correct\"],\n",
    "        \"set\": word[\"set\"],\n",
    "        \"setRun\": set_run\n",
    "    } for word in shuffled_list]\n",
    "\n",
    "# Placeholder for all JS entries\n",
    "all_entries = []\n",
    "\n",
    "# Loop over each CSV file and set name\n",
    "for i in range(4):\n",
    "    filename = f'group_{i+1}_word_pairs.csv'\n",
    "    set_name = f\"Set{chr(65 + i)}\"  # 'SetA', 'SetB', 'SetC', 'SetD'\n",
    "    \n",
    "    with open(filename, newline='') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        csv_content = list(csvreader)  # Convert csvreader to a list\n",
    "        js_entries = csv_to_js_format(csv_content, set_name)  # Pass csv_content and set_name\n",
    "        for run_number in range(1, 5):  # Create 4 runs for each set\n",
    "            shuffled_set = set_shuffle(js_entries, set_name, run_number)\n",
    "            all_entries.extend(shuffled_set)\n",
    "\n",
    "# Save all entries into the JS file\n",
    "stimuli_js_content = \"let trial_objects = \" + json.dumps(all_entries, indent=4) + \";\"\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(stimuli_js_content)\n",
    "\n",
    "# Count occurrences of each id... okay well this isn't right but also not really important, so I'll leave it as is. What's happening is that the id is being counted but the assignment is made during a loop and so for each id there is 4 unique string pairs. for example 032 is about, water, birth, and trust.\n",
    "id_counter = Counter(entry[\"id\"] for entry in all_entries)\n",
    "print(\"ID Occurrences:\")\n",
    "for id_, count in id_counter.items():\n",
    "    print(f\"id {id_}: {count} times\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caac0b83",
   "metadata": {},
   "source": [
    "Okay I'm just looking for a sanity check and gonna run a loop over the stimuli js file to correct the  ID and then see if that does it. I think this notebook should be a code review for lab someday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0785e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the stimuli file\n",
    "with open(\"stimuli.js\", \"r\") as file:\n",
    "    stimuli_js_content = file.read()\n",
    "\n",
    "# Extract the JSON data from the stimuli file\n",
    "json_data = json.loads(stimuli_js_content[len(\"let trial_objects = \"):-1])\n",
    "\n",
    "# Create a mapping for unique anagrams to new IDs\n",
    "unique_anagram_to_id = {}\n",
    "id_counter = 1\n",
    "\n",
    "for entry in json_data:\n",
    "    anagram = entry[\"anagram\"]\n",
    "    if anagram not in unique_anagram_to_id:\n",
    "        unique_anagram_to_id[anagram] = f\"{id_counter:03d}\"\n",
    "        id_counter += 1\n",
    "\n",
    "# Reassign IDs in the JSON data\n",
    "for entry in json_data:\n",
    "    entry[\"id\"] = unique_anagram_to_id[entry[\"anagram\"]]\n",
    "\n",
    "# Save the updated entries into the JS file\n",
    "updated_stimuli_js_content = \"let trial_objects = \" + json.dumps(json_data, indent=4) + \";\"\n",
    "with open(\"updated_stimuli.js\", \"w\") as file:\n",
    "    file.write(updated_stimuli_js_content)\n",
    "\n",
    "# Print the new ID mapping for verification\n",
    "print(\"New ID mapping for unique anagrams:\")\n",
    "for anagram, new_id in unique_anagram_to_id.items():\n",
    "    print(f\"Anagram: {anagram}, New ID: {new_id}\")\n",
    "\n",
    "# This worked okay lets save to the stimuli.js file\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(updated_stimuli_js_content)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python (miniconda3)",
   "language": "python",
   "name": "miniconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
