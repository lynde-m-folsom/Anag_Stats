{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cca7df1d",
   "metadata": {},
   "source": [
    "<h2> Generating the Stimuli file</h2>\n",
    "    \n",
    "This notebook is a modified version of prior work to generate the stimuli files used in anagram experiments.\n",
    "\n",
    "First we grab a list of words and then shuffle. From the shuffled set we then set up seperate groups (set groups) from there we use those paired shuffle and \"correct\" words to make the stimuli file used in the anagram experiments. \n",
    "\n",
    "This one will differ since I'll be including the sources and frequency data on use of the words in the set. Furthermore, I will be using this notebook to also make a json object of valid solutions for each shuffle. The stimuli file we use includes the word used for shuffling however, those shuffled strings can be solved to more than one real english word which is how we define \"valid\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ce3545",
   "metadata": {},
   "source": [
    "We can use dictionaries that have all the words of specific strings than use a function to give us some number of those words. \n",
    "\n",
    "#### The word bank we are using is from Word Net:\n",
    "    George A. Miller (1995). WordNet: A Lexical Database for English.\n",
    "    Communications of the ACM Vol. 38, No. 11: 39-41.\n",
    "    Christiane Fellbaum (1998, ed.) WordNet: An Electronic Lexical Database. Cambridge, MA: MIT Press.\n",
    "    WordNet: An Electronic Lexical Database\n",
    "\n",
    "#### The frequency information is from the word freq: \n",
    "    Robyn Speer. (2022). rspeer/wordfreq: v3.0 (v3.0.2). Zenodo. https://doi.org/10.5281/zenodo.7199437\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c67cb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lyndefolsom/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "from propernoun import remove_proper_nouns\n",
    "from anagram_utils import (\n",
    "    mk_dict_from_wordnet_for_length,\n",
    "    get_word_frequencies,\n",
    "    sort_words_by_frequency,\n",
    "    get_top_n_words,\n",
    "    \n",
    ")\n",
    "nltk.download(\"wordnet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e446f8",
   "metadata": {},
   "source": [
    "A code chunk for script testing (bc lynde is scared of using their terminal and messing with their enviornment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d44ce8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tag import pos_tag\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "test_words = ['george', 'jesus', 'was', 'the', 'first', 'president', 'of', 'the', 'united', 'states']\n",
    "good_words = remove_proper_nouns(test_words)\n",
    "print(good_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387d166f",
   "metadata": {},
   "source": [
    "<h3> Making the dictionary </h3>\n",
    "Using the utility script we make a dictionary for the words we want to use. The function mk_dict... will look for a wordlength and call up the wordnet dictionary to use for the stimuli generation. \n",
    "\n",
    "Still debugging the remove proper nouns function @to-do \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03148bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 2310 words of length 4\n",
      "found 4095 words of length 5\n",
      "found 6258 words of length 6\n",
      "found 2234 words of length 4\n",
      "found 3982 words of length 5\n",
      "found 6071 words of length 6\n"
     ]
    }
   ],
   "source": [
    "word_dict = {}\n",
    "for wordlength in [4,5,6]:\n",
    "    word_dict[wordlength] = mk_dict_from_wordnet_for_length(wordlength)\n",
    "    print(f'found {len(word_dict[wordlength])} words of length {wordlength}')\n",
    "\n",
    "# making a loop through the word dic to use the remove proper nouns function\n",
    "for wordlength in word_dict:\n",
    "    word_dict[wordlength] = remove_proper_nouns(word_dict[wordlength])\n",
    "    print(f'found {len(word_dict[wordlength])} words of length {wordlength}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71eeb548",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = get_word_frequencies(word_dict)\n",
    "sorted_wordlist = sort_words_by_frequency(word_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f187df4b",
   "metadata": {},
   "source": [
    "## Dictionary made, lets get our words\n",
    "I made a get words function to grab the list I want based off how many grams we want to have at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee42a10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the list we want, so for each column of the sorted word list grab the first 200\n",
    "  # Convert each value (which should be a list) to a list\n",
    "column_lists = [list(value) for value in sorted_wordlist.values()]\n",
    "  # Combine these lists into a list of lists\n",
    "sorted_wordlist = list(zip(*column_lists))\n",
    "\n",
    "top_words = get_top_n_words(sorted_wordlist, 200)\n",
    "full_list = [word for sublist in top_words for word in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a39620e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#make function to shuffle the letters and append to the list\n",
    "def shuffle_letters(word):\n",
    "    shuffled = list(word)\n",
    "    random.shuffle(shuffled)\n",
    "    shuffled_word = \"\".join(shuffled)\n",
    "    return shuffled_word\n",
    "\n",
    "# make a function that takes a list uses the shuffle letters function and returns a list of shuffled words\n",
    "def shuffle_list(word_list):\n",
    "    shuffled_list = []\n",
    "    for word in word_list:\n",
    "        shuffled_list.append(shuffle_letters(word))\n",
    "    return shuffled_list\n",
    "\n",
    "\n",
    "full_list_shuffled = shuffle_list(full_list)\n",
    "#cat_full_list = list(zip(full_list, full_list_shuffled))\n",
    "cat_full_list = pd.DataFrame({\n",
    "    'root'  : full_list,\n",
    "    'shuffled' : full_list_shuffled\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882fe757",
   "metadata": {},
   "source": [
    "## Make a solution key\n",
    "\n",
    "Many anagrams have more than one response that is a valid solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd284b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the the fivestring, sixstring, and fourstring dictionaries we will make a function to check if a word is in the dictionary and return the definition of the word if it is.\n",
    "# def check_word(word):\n",
    "#     if word in fivestrings:\n",
    "#         return f\"{word}: {fivestrings[word]}\"\n",
    "#     elif word in sixstrings:\n",
    "#         return f\"{word}: {sixstrings[word]}\"\n",
    "#     elif word in fourstrings:\n",
    "#         return f\"{word}: {fourstrings[word]}\"\n",
    "#     else:\n",
    "#         return f\"{word} is not in the dictionary\"\n",
    "\n",
    "\n",
    "# that function we can embed in a loop that checks if a permutation is in our dictionary.\n",
    "# we will need to first keep each string list separate to then reorder the letters and then check if that is a word, if so we we add it, otherwise we move to next word\n",
    "# we will make a permutation such that \"baout\" will be arranged to \"outba\" check if that's a word, if not move to next permutation\n",
    "import itertools\n",
    "\n",
    "\n",
    "# def find_valid_words(input_string):\n",
    "#     valid_words = []\n",
    "#     permutations = itertools.permutations(input_string)\n",
    "\n",
    "#     for perm in permutations:\n",
    "#         perm_word = \"\".join(perm)\n",
    "#         result = check_word(perm_word)\n",
    "#         if \"is not in the dictionary\" not in result:\n",
    "#             valid_words.append(perm_word)\n",
    "\n",
    "#     return valid_words\n",
    "\n",
    "\n",
    "# now we go through the list of shuffled words use the find_valid_words for each word and append the results to a list\n",
    "# five_possible_words = []\n",
    "# for word in shuffled_five:\n",
    "#     possible_words = find_valid_words(word)\n",
    "#     five_possible_words.append(possible_words)\n",
    "# six_possible_words = []\n",
    "# for word in shuffled_six:\n",
    "#     possible_words = find_valid_words(word)\n",
    "#     six_possible_words.append(possible_words)\n",
    "# four_possible_words = []\n",
    "# for word in shuffled_fours:\n",
    "#     possible_words = find_valid_words(word)\n",
    "#     four_possible_words.append(possible_words)\n",
    "\n",
    "# # aggregate all of these lists into one list\n",
    "# all_possible_words = five_possible_words + six_possible_words + four_possible_words\n",
    "# # convert the list into a dictionary with the shuffled word as the key and the list of possible words as the value\n",
    "# possible_words_dict = dict(\n",
    "#     zip(shuffled_five + shuffled_six + shuffled_fours, all_possible_words)\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa26ca32",
   "metadata": {},
   "source": [
    "## With an All Possible file we can check valid\n",
    "Now that we have all the possible words that can be made from a particular string, we can use that dictionary as a json that could be read into the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e541ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the possible_words_dict into a json file in which the key is the shuffled word and the value is the list of possible words\n",
    "# all_possible_words_json = json.dumps(possible_words_dict)\n",
    "# with open(\"possible_words.json\", \"w\") as file:\n",
    "#     file.write(all_possible_words_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf7d84",
   "metadata": {},
   "source": [
    "<h3> Concatinate and create the Sets </h3>\n",
    "\n",
    "So now we need to make the stimuli for each of the blocks. For this we want to have 10 strings of each length per block for 3 blocks. \n",
    "\n",
    "And then we make groups of those until all the words are assigned. Finally, we will collect 10-30 participants per group. \n",
    "\n",
    "So in total: 3 blocks of 30 anagrams which will be 10 four letters, 10 five letters, 10 six letters. To make sure we get all the words, we will need 4 groups to have the 120 words for each string length represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3f61ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a distributing function to spread the words across the four groups\n",
    "def distribute_words(word_list, num_groups=4):\n",
    "    # Calculate the number of words per group\n",
    "    group_size = len(word_list) // num_groups\n",
    "\n",
    "    # Create groups\n",
    "    groups = [[] for _ in range(num_groups)]\n",
    "\n",
    "    # Distribute words to each group\n",
    "    for index, word in enumerate(word_list):\n",
    "        group_index = index // group_size\n",
    "        if (\n",
    "            group_index < num_groups\n",
    "        ):  # This check prevents index out of range if not perfectly divisible\n",
    "            groups[group_index].append(word)\n",
    "\n",
    "    return groups\n",
    "\n",
    "\n",
    "# Distribute words to the groups\n",
    "# grouped_four_letter_words = distribute_words(shuffled_four_pairs)\n",
    "# grouped_five_letter_words = distribute_words(shuffled_pairs_five)\n",
    "# grouped_six_letter_words = distribute_words(shuffled_six_pairs)\n",
    "\n",
    "# Save each group to a separate CSV file-- we do this so we can review the words in each group and how they are distributed.\n",
    "# for i in range(4):\n",
    "#     filename = f\"group_{i+1}_word_pairs.csv\"\n",
    "#     with open(filename, \"w\", newline=\"\") as file:\n",
    "#         writer = csv.writer(file)\n",
    "#         writer.writerow([\"Type\", \"Word Pairs\"])\n",
    "#         writer.writerows(\n",
    "#             [[\"Four-Letter\", word] for word in grouped_four_letter_words[i]]\n",
    "#         )\n",
    "#         writer.writerows(\n",
    "#             [[\"Five-Letter\", word] for word in grouped_five_letter_words[i]]\n",
    "#         )\n",
    "#         writer.writerows([[\"Six-Letter\", word] for word in grouped_six_letter_words[i]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7555b9ca",
   "metadata": {},
   "source": [
    "Okay now we need to create the stimuli file which should be in a .js that will look like: \n",
    "\n",
    "let trial_objects = [\n",
    "    {\n",
    "        \"id\": \"001\",\n",
    "        \"type\": \"Four-Letter\",\n",
    "        \"anagram\": \"atth\",\n",
    "        \"correct\": \"that\",\n",
    "        \"set\": \"A\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bba816",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_directory = (\n",
    "    \"./group_1_word_pairs.csv\",\n",
    "    \"./group_2_word_pairs.csv\",\n",
    "    \"./group_3_word_pairs.csv\",\n",
    "    \"./group_4_word_pairs.csv\",\n",
    ")\n",
    "\n",
    "\n",
    "# Function to convert CSV content to JS format\n",
    "def csv_to_js_format(csv_content, set_name):\n",
    "    js_entries = []\n",
    "    for idx, row in enumerate(csv_content):\n",
    "        word_type, word_pair = row\n",
    "        if idx == 0:  # Skip header row\n",
    "            continue\n",
    "        original, anagram = word_pair.split(\", \")\n",
    "        js_entry = {\n",
    "            \"id\": f\"{str(idx).zfill(3)}\",\n",
    "            \"type\": word_type,\n",
    "            \"anagram\": anagram,\n",
    "            \"correct\": original,\n",
    "            \"set\": set_name,\n",
    "        }\n",
    "        js_entries.append(js_entry)\n",
    "    return js_entries\n",
    "\n",
    "\n",
    "# Placeholder for all JS entries\n",
    "all_entries = []\n",
    "\n",
    "# Loop over each CSV file and set name\n",
    "for i in range(4):\n",
    "    filename = f\"group_{i+1}_word_pairs.csv\"\n",
    "    set_name = f\"Set{chr(65 + i)}\"  # 'SetA', 'SetB', 'SetC', 'SetD'\n",
    "\n",
    "    with open(filename, newline=\"\") as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        js_entries = csv_to_js_format(\n",
    "            csvreader, set_name\n",
    "        )  # Pass csvreader and set_name\n",
    "        all_entries.extend(js_entries)\n",
    "\n",
    "## Writing the JS file\n",
    "# Save all entries into the JS file\n",
    "stimuli_js_content = \"let trial_objects = \" + str(all_entries) + \";\"\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(stimuli_js_content)\n",
    "\n",
    "# modfiying the stimuli.js file to be in the preferred format (mostly for readability)\n",
    "# Here we format the JSON with specific spacing and bracketing style\n",
    "stimuli_js_content = \"let trial_objects = [\\n\"\n",
    "for entry in all_entries:\n",
    "    stimuli_js_content += \"    \" + json.dumps(entry, indent=4) + \",\\n\"\n",
    "stimuli_js_content = stimuli_js_content.rstrip(\",\\n\") + \"\\n];\"\n",
    "\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(stimuli_js_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a2c41",
   "metadata": {},
   "source": [
    "Alright we've now got a set but we are gonna take each set and then shuffle into 4 coded runs.\n",
    "Essentially, by trying to randomize the order on the fly, we can introduce a bunch of bugs into the timeline variables. \n",
    "By hardcoding 4 unique run orders for each set of words, we are addressing order effects without risking more bugs. \n",
    "\n",
    "So below we take the stimuli file, filter by set, add a new parameter to the stimuli file which is its run order assignment which is numbered 1-4. \n",
    "Now ALL of the words in set A will be shuffled into 4 unique orders and assigned to A1, A2, A3, A4. All the A words are the same but their order is now randomized.\n",
    "The this will be saved into the stimuli file as \"SetRun\":\"A1\" etc etc.\n",
    "\n",
    "Now this will make the Stimuli.js file increase substancially (by four) but will keep our code clean and modular. Also fewer headaches since the other option is adjusting the JS utility file that makes the variable order and we really don't wanna do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afebca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert CSV content to JS format\n",
    "def csv_to_js_format(csv_content, set_name):\n",
    "    js_entries = []\n",
    "    for idx, row in enumerate(csv_content):\n",
    "        if idx == 0:  # Skip header row\n",
    "            continue\n",
    "        word_type, word_pair = row\n",
    "        original, anagram = word_pair.split(\", \")\n",
    "        js_entry = {\n",
    "            \"id\": f\"{str(idx + 1).zfill(3)}\",  # Assign unique IDs starting from 001\n",
    "            \"type\": word_type,\n",
    "            \"anagram\": anagram,\n",
    "            \"correct\": original,\n",
    "            \"set\": set_name,\n",
    "        }\n",
    "        js_entries.append(js_entry)\n",
    "    return js_entries\n",
    "\n",
    "\n",
    "# Function to shuffle a set of words and include set run order\n",
    "def set_shuffle(word_list, set_name, run_number):\n",
    "    shuffled_list = word_list[:]  # Create a copy of the word_list to shuffle\n",
    "    random.shuffle(shuffled_list)  # Shuffle the list of words\n",
    "    set_run = f\"{set_name}{run_number}\"\n",
    "    return [\n",
    "        {\n",
    "            \"id\": word[\"id\"],\n",
    "            \"type\": word[\"type\"],\n",
    "            \"anagram\": word[\"anagram\"],\n",
    "            \"correct\": word[\"correct\"],\n",
    "            \"set\": word[\"set\"],\n",
    "            \"setRun\": set_run,\n",
    "        }\n",
    "        for word in shuffled_list\n",
    "    ]\n",
    "\n",
    "\n",
    "# Placeholder for all JS entries\n",
    "all_entries = []\n",
    "\n",
    "# Loop over each CSV file and set name\n",
    "for i in range(4):\n",
    "    filename = f\"group_{i+1}_word_pairs.csv\"\n",
    "    set_name = f\"Set{chr(65 + i)}\"  # 'SetA', 'SetB', 'SetC', 'SetD'\n",
    "\n",
    "    with open(filename, newline=\"\") as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        csv_content = list(csvreader)  # Convert csvreader to a list\n",
    "        js_entries = csv_to_js_format(\n",
    "            csv_content, set_name\n",
    "        )  # Pass csv_content and set_name\n",
    "        for run_number in range(1, 5):  # Create 4 runs for each set\n",
    "            shuffled_set = set_shuffle(js_entries, set_name, run_number)\n",
    "            all_entries.extend(shuffled_set)\n",
    "\n",
    "# Save all entries into the JS file\n",
    "stimuli_js_content = \"let trial_objects = \" + json.dumps(all_entries, indent=4) + \";\"\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(stimuli_js_content)\n",
    "\n",
    "# Count occurrences of each id... okay well this isn't right but also not really important, so I'll leave it as is. What's happening is that the id is being counted but the assignment is made during a loop and so for each id there is 4 unique string pairs. for example 032 is about, water, birth, and trust.\n",
    "id_counter = Counter(entry[\"id\"] for entry in all_entries)\n",
    "print(\"ID Occurrences:\")\n",
    "for id_, count in id_counter.items():\n",
    "    print(f\"id {id_}: {count} times\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caac0b83",
   "metadata": {},
   "source": [
    "Okay I'm just looking for a sanity check and gonna run a loop over the stimuli js file to correct the  ID and then see if that does it. I think this notebook should be a code review for lab someday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0785e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the stimuli file\n",
    "with open(\"stimuli.js\", \"r\") as file:\n",
    "    stimuli_js_content = file.read()\n",
    "\n",
    "# Extract the JSON data from the stimuli file\n",
    "json_data = json.loads(stimuli_js_content[len(\"let trial_objects = \") : -1])\n",
    "\n",
    "# Create a mapping for unique anagrams to new IDs\n",
    "unique_anagram_to_id = {}\n",
    "id_counter = 1\n",
    "\n",
    "for entry in json_data:\n",
    "    anagram = entry[\"anagram\"]\n",
    "    if anagram not in unique_anagram_to_id:\n",
    "        unique_anagram_to_id[anagram] = f\"{id_counter:03d}\"\n",
    "        id_counter += 1\n",
    "\n",
    "# Reassign IDs in the JSON data\n",
    "for entry in json_data:\n",
    "    entry[\"id\"] = unique_anagram_to_id[entry[\"anagram\"]]\n",
    "\n",
    "# Save the updated entries into the JS file\n",
    "updated_stimuli_js_content = (\n",
    "    \"let trial_objects = \" + json.dumps(json_data, indent=4) + \";\"\n",
    ")\n",
    "with open(\"updated_stimuli.js\", \"w\") as file:\n",
    "    file.write(updated_stimuli_js_content)\n",
    "\n",
    "# Print the new ID mapping for verification\n",
    "print(\"New ID mapping for unique anagrams:\")\n",
    "for anagram, new_id in unique_anagram_to_id.items():\n",
    "    print(f\"Anagram: {anagram}, New ID: {new_id}\")\n",
    "\n",
    "# This worked okay lets save to the stimuli.js file\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(updated_stimuli_js_content)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python (miniconda3)",
   "language": "python",
   "name": "miniconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
