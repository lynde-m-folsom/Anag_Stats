{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cca7df1d",
   "metadata": {},
   "source": [
    "<h2> How to get the word strings code </h2>\n",
    "    \n",
    "This script gives us the words and then shuffles the sequences of the letters. The shuffling is added next to the solution for easy scripting down the line. \n",
    "\n",
    "The code chunks are compiled by Lynde@ but generated with assistance from GPT-4. This markdown shows in comments what are human and what are AI,  affectionatly named \"pelops\" and then numbered with each restart (as for generating some of the code the AI would loop and crash itself). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387d166f",
   "metadata": {},
   "source": [
    "<h3> Five letter words </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c67cb28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['about, otbau',\n",
       " 'above, beoav',\n",
       " 'after, eatrf',\n",
       " 'again, ginaa',\n",
       " 'below, oblwe',\n",
       " 'could, dolcu',\n",
       " 'every, evyre',\n",
       " 'first, sitrf',\n",
       " 'found, ofndu',\n",
       " 'great, gtrae',\n",
       " 'house, husoe',\n",
       " 'large, grael',\n",
       " 'learn, eanrl',\n",
       " 'never, eenrv',\n",
       " 'other, hotre',\n",
       " 'place, leacp',\n",
       " 'plant, latnp',\n",
       " 'point, ponti',\n",
       " 'right, gthir',\n",
       " 'small, lmsal',\n",
       " 'sound, onusd',\n",
       " 'spell, llpse',\n",
       " 'still, lilts',\n",
       " 'study, ysdtu',\n",
       " 'their, iethr',\n",
       " 'there, trehe',\n",
       " 'these, sheet',\n",
       " 'thing, hgtin',\n",
       " 'think, ktihn',\n",
       " 'three, eehrt',\n",
       " 'water, aretw',\n",
       " 'where, ewrhe',\n",
       " 'which, wihhc',\n",
       " 'world, wrdol',\n",
       " 'would, odlwu',\n",
       " 'write, twire',\n",
       " 'young, oyugn',\n",
       " 'quick, kuicq',\n",
       " 'quiet, iuteq',\n",
       " 'quite, uqiet',\n",
       " 'brown, nowbr',\n",
       " 'clear, aecrl',\n",
       " 'black, aklbc',\n",
       " 'white, ehwti',\n",
       " 'green, gener',\n",
       " 'super, ruesp',\n",
       " 'sweet, weset',\n",
       " 'stand, sdnat',\n",
       " 'start, atrts',\n",
       " 'state, tsaet',\n",
       " 'story, yotsr',\n",
       " 'light, htigl',\n",
       " 'might, mihgt',\n",
       " 'night, tnigh',\n",
       " 'right, hgtir',\n",
       " 'sight, higts',\n",
       " 'those, hoest',\n",
       " 'under, nderu',\n",
       " 'while, ielhw',\n",
       " 'angry, rngay',\n",
       " 'birth, rthib',\n",
       " 'death, dheta',\n",
       " 'earth, ehrat',\n",
       " 'final, nilfa',\n",
       " 'fresh, srhfe',\n",
       " 'heart, eahtr',\n",
       " 'month, month',\n",
       " 'north, htorn',\n",
       " 'party, ypatr',\n",
       " 'peace, ceeap',\n",
       " 'piece, ecipe',\n",
       " 'proud, odrpu',\n",
       " 'sense, enses',\n",
       " 'seven, ensev',\n",
       " 'shall, alshl',\n",
       " 'short, othrs',\n",
       " 'since, inecs',\n",
       " 'sixth, shxti',\n",
       " 'thank, nhakt',\n",
       " 'their, htire',\n",
       " 'thing, ghnti',\n",
       " 'think, nithk',\n",
       " 'third, rtidh',\n",
       " 'three, erhte',\n",
       " 'today, datoy',\n",
       " 'total, oattl',\n",
       " 'touch, utcoh',\n",
       " 'train, arint',\n",
       " 'treat, rteta',\n",
       " 'trial, lrati',\n",
       " 'trust, truts',\n",
       " 'truth, rtuht',\n",
       " 'twice, tcwie',\n",
       " 'under, eunrd',\n",
       " 'until, ituln',\n",
       " 'usual, uausl',\n",
       " 'value, auvel',\n",
       " 'video, oveid',\n",
       " 'visit, iitsv',\n",
       " 'voice, oevci',\n",
       " 'waste, waets',\n",
       " 'watch, twhac',\n",
       " 'water, rtewa',\n",
       " 'while, hiwel',\n",
       " 'white, wieht',\n",
       " 'whole, eowhl',\n",
       " 'woman, omnaw',\n",
       " 'women, wnmeo',\n",
       " 'world, lodwr',\n",
       " 'worry, rwoyr',\n",
       " 'worse, erwso',\n",
       " 'worth, wtroh',\n",
       " 'write, iwrte',\n",
       " 'wrong, gowrn',\n",
       " 'young, yngou',\n",
       " 'youth, tuyho',\n",
       " 'apple, lpaep',\n",
       " 'chair, rhaic',\n",
       " 'clock, olkcc',\n",
       " 'shelf, hself']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this code chunk was generated by GPT-4.0 from \"Open\"Ai\n",
    "import random\n",
    "\n",
    "# List of commonly used 5-letter English words\n",
    "five_letter_words = [\n",
    "    \"about\", \"above\", \"after\", \"again\", \"below\", \"could\", \"every\", \"first\",\n",
    "    \"found\", \"great\", \"house\", \"large\", \"learn\", \"never\", \"other\", \"place\",\n",
    "    \"plant\", \"point\", \"right\", \"small\", \"sound\", \"spell\", \"still\", \"study\",\n",
    "    \"their\", \"there\", \"these\", \"thing\", \"think\", \"three\", \"water\", \"where\",\n",
    "    \"which\", \"world\", \"would\", \"write\", \"young\", \"quick\", \"quiet\", \"quite\",\n",
    "    \"brown\", \"clear\", \"black\", \"white\", \"green\", \"super\", \"sweet\", \"stand\",\n",
    "    \"start\", \"state\", \"story\", \"light\", \"might\", \"night\", \"right\", \"sight\",\n",
    "    \"those\", \"under\", \"while\", \"angry\", \"birth\", \"death\", \"earth\", \"final\",\n",
    "    \"fresh\", \"heart\", \"month\", \"north\", \"party\", \"peace\", \"piece\", \"proud\",\n",
    "    \"sense\", \"seven\", \"shall\", \"short\", \"since\", \"sixth\", \"thank\", \"their\",\n",
    "    \"thing\", \"think\", \"third\", \"three\", \"today\", \"total\", \"touch\", \"train\",\n",
    "    \"treat\", \"trial\", \"trust\", \"truth\", \"twice\", \"under\", \"until\", \"usual\",\n",
    "    \"value\", \"video\", \"visit\", \"voice\", \"waste\", \"watch\", \"water\", \"while\",\n",
    "    \"white\", \"whole\", \"woman\", \"women\", \"world\", \"worry\", \"worse\", \"worth\",\n",
    "    \"write\", \"wrong\", \"young\", \"youth\", \"apple\", \"chair\", \"clock\", \"shelf\",\n",
    "    \"table\", \"house\", \"money\", \"photo\", \"sugar\", \"honey\", \"drama\", \"fifty\"\n",
    "]\n",
    "\n",
    "# Shuffle each word's letters and pair them together\n",
    "shuffled_pairs_five = []\n",
    "for word in five_letter_words:\n",
    "    shuffled = list(word)\n",
    "    random.shuffle(shuffled)\n",
    "    shuffled_word = ''.join(shuffled)\n",
    "    shuffled_pairs_five.append(f\"{word}, {shuffled_word}\")\n",
    "    #set to 120 words\n",
    "shuffled_pairs_five =  shuffled_pairs_five[:120]\n",
    "\n",
    "shuffled_pairs_five[:]  # Show the first 120 pairs as requested (list is exactly 120 words long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554a050",
   "metadata": {},
   "source": [
    "<h3> Six letter words and shuffled </h3>\n",
    "\n",
    "Six letter words were slightly more challenging but made in the same way as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a39620e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##this code chunk was generated by GPT-4.0 from \"Open\"Ai\n",
    "import random\n",
    "# List of commonly used 6-letter English words\n",
    "six_letter_words = [\n",
    "    \"action\", \"beauty\", \"change\", \"damage\", \"effect\", \"flight\", \"growth\", \"health\",\n",
    "    \"island\", \"jungle\", \"kernel\", \"liquid\", \"method\", \"nature\", \"object\", \"people\",\n",
    "    \"quartz\", \"record\", \"silver\", \"travel\", \"useful\", \"vacuum\", \"wealth\", \"yellow\",\n",
    "    \"zephyr\", \"abroad\", \"breeze\", \"circle\", \"desert\", \"energy\", \"father\", \"garden\",\n",
    "    \"hammer\", \"injury\", \"jacket\", \"killer\", \"laptop\", \"mirror\", \"needle\", \"office\",\n",
    "    \"pencil\", \"quiver\", \"reason\", \"spirit\", \"temple\", \"update\", \"vision\", \"wallet\",\n",
    "    \"xenial\", \"yearly\", \"zodiac\", \"afford\", \"branch\", \"charge\", \"danger\", \"empire\",\n",
    "    \"fossil\", \"guitar\", \"harbor\", \"import\", \"juggle\", \"kitten\", \"league\", \"market\",\n",
    "    \"narrow\", \"orange\", \"patent\", \"quaint\", \"return\", \"simple\", \"tissue\", \"urgent\",\n",
    "    \"violet\", \"wrench\", \"yellow\", \"zombie\", \"amount\", \"beacon\", \"candle\", \"decade\",\n",
    "    \"export\", \"frozen\", \"gossip\", \"honest\", \"impact\", \"junior\", \"kidney\", \"legend\",\n",
    "    \"magnet\", \"novice\", \"option\", \"pillar\", \"quorum\", \"recipe\", \"sponge\", \"target\",\n",
    "    \"unique\", \"valley\", \"wonder\", \"youths\", \"zigzag\", \"admire\", \"branch\", \"choice\",\n",
    "    \"damage\", \"exotic\", \"family\", \"global\", \"heroic\", \"island\", \"joyful\", \"knotty\",\n",
    "    \"little\", \"medium\", \"normal\", \"online\", \"prince\", \"square\", \"tongue\", \"update\",\n",
    "    \"visual\", \"window\", \"yacht\", \"zodiac\"\n",
    "]\n",
    "\n",
    "# Shuffle each word's letters and pair them together\n",
    "shuffled_six_pairs = []\n",
    "for word in six_letter_words:\n",
    "    shuffled = list(word)\n",
    "    random.shuffle(shuffled)\n",
    "    shuffled_word = ''.join(shuffled)\n",
    "    shuffled_six_pairs.append(f\"{word}, {shuffled_word}\")\n",
    "    # set the count to 120\n",
    "    shuffled_six_pairs= shuffled_six_pairs[:120]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c0cb7f",
   "metadata": {},
   "source": [
    "<h3> Four letter words </h3>\n",
    "    \n",
    "Significantly more challenging. Many four letter words can unshuffle into multiple answers, furthermore, shuffling the order can result in the exact same word as the answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d1fe59",
   "metadata": {},
   "source": [
    "<h3> Finaly the four letter words </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83d1d927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['that, ttha',\n",
       " 'with, hwti',\n",
       " 'this, itsh',\n",
       " 'from, mfor',\n",
       " 'your, oruy',\n",
       " 'have, heav',\n",
       " 'they, tyeh',\n",
       " 'will, iwll',\n",
       " 'what, hwat',\n",
       " 'when, enhw',\n",
       " 'then, enht',\n",
       " 'them, hemt',\n",
       " 'some, osme',\n",
       " 'take, tkea',\n",
       " 'into, itno',\n",
       " 'time, mite',\n",
       " 'like, lkie',\n",
       " 'more, omer',\n",
       " 'very, eryv',\n",
       " 'know, konw',\n",
       " 'than, tahn',\n",
       " 'want, antw',\n",
       " 'text, txet',\n",
       " 'back, akbc',\n",
       " 'give, vgie',\n",
       " 'most, stmo',\n",
       " 'also, saol',\n",
       " 'down, wdno',\n",
       " 'over, eorv',\n",
       " 'much, uhcm',\n",
       " 'need, dnee',\n",
       " 'feel, eelf',\n",
       " 'come, eocm',\n",
       " 'love, elvo',\n",
       " 'play, paly',\n",
       " 'find, ndif',\n",
       " 'move, vome',\n",
       " 'even, eevn',\n",
       " 'live, leiv',\n",
       " 'help, phel',\n",
       " 'line, niel',\n",
       " 'work, okrw',\n",
       " 'part, tpra',\n",
       " 'next, enxt',\n",
       " 'call, lalc',\n",
       " 'talk, atkl',\n",
       " 'plus, uspl',\n",
       " 'away, awya',\n",
       " 'sure, srue',\n",
       " 'seem, emse',\n",
       " 'stay, atsy',\n",
       " 'turn, rnut',\n",
       " 'hard, rdah',\n",
       " 'page, agpe',\n",
       " 'read, erad',\n",
       " 'left, eflt',\n",
       " 'stop, psto',\n",
       " 'keep, ekpe',\n",
       " 'face, ecfa',\n",
       " 'city, yict',\n",
       " 'best, btse',\n",
       " 'tell, lelt',\n",
       " 'hear, ehar',\n",
       " 'hope, oeph',\n",
       " 'life, feil',\n",
       " 'cost, tsco',\n",
       " 'wait, itaw',\n",
       " 'plan, npal',\n",
       " 'true, eurt',\n",
       " 'many, amyn',\n",
       " 'idea, ieda',\n",
       " 'body, yodb',\n",
       " 'info, ifon',\n",
       " 'game, ameg',\n",
       " 'care, reac',\n",
       " 'last, lsat',\n",
       " 'team, emta',\n",
       " 'week, ekew',\n",
       " 'note, eotn',\n",
       " 'show, owsh',\n",
       " 'able, lbea',\n",
       " 'hand, dahn',\n",
       " 'view, iwve',\n",
       " 'home, mhoe',\n",
       " 'site, stei',\n",
       " 'case, sace',\n",
       " 'most, mtos',\n",
       " 'road, orad',\n",
       " 'test, estt',\n",
       " 'copy, opcy',\n",
       " 'step, etps',\n",
       " 'user, sure',\n",
       " 'baby, aybb',\n",
       " 'base, aebs',\n",
       " 'miss, msis',\n",
       " 'love, vleo',\n",
       " 'risk, srki',\n",
       " 'fish, shif',\n",
       " 'blue, buel',\n",
       " 'form, rfom',\n",
       " 'pass, saps',\n",
       " 'camp, acpm',\n",
       " 'lead, edal',\n",
       " 'sale, elas',\n",
       " 'club, lbcu',\n",
       " 'army, ryma',\n",
       " 'band, bnda',\n",
       " 'bill, lilb',\n",
       " 'mass, ssma',\n",
       " 'card, rcad',\n",
       " 'list, tsli',\n",
       " 'slip, pisl',\n",
       " 'tree, eert',\n",
       " 'race, rcea',\n",
       " 'ball, labl',\n",
       " 'sink, kins',\n",
       " 'coat, oact',\n",
       " 'milk, ilmk',\n",
       " 'sand, asdn',\n",
       " 'jazz, azjz']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Pelops 3:\n",
    "four_letter_words = [\n",
    "    'that', 'with', 'this', 'from', 'your', 'have', 'they', 'will', 'what', 'when', 'then', 'them', 'some', 'take', 'into',\n",
    "    'time', 'look', 'like', 'more', 'very', 'know', 'than', 'want', 'text', 'back', 'good', 'give', 'most', 'also', 'down',\n",
    "    'over', 'much', 'need', 'feel', 'come', 'love', 'play', 'find', 'move', 'even', 'live', 'help', 'line', 'work', 'part',\n",
    "    'next', 'call', 'talk', 'plus', 'away', 'sure', 'seem', 'food', 'stay', 'turn', 'hard', 'page', 'read', 'left', 'soon',\n",
    "    'stop', 'keep', 'face', 'city', 'best', 'tell', 'hear', 'hope', 'room', 'door', 'life', 'cost', 'wait', 'plan', 'true',\n",
    "    'many', 'idea', 'body', 'info', 'book', 'game', 'care', 'last', 'team', 'week', 'note', 'show', 'able', 'hand', 'view',\n",
    "    'home', 'site', 'case', 'most', 'road', 'test', 'copy', 'step', 'user', 'baby', 'base', 'miss', 'love', 'risk', 'fish',\n",
    "    'blue', 'form', 'pass', 'camp', 'lead', 'sale', 'club', 'army', 'band', 'pool', 'bill', 'foot', 'mass', 'card', 'list',\n",
    "    'slip', 'tree', 'race', 'ball', 'sink', 'coat', 'milk', 'sand', 'jazz', 'type', 'film', 'burn', 'bolt', 'golf', 'wolf',\n",
    "    'lamp', 'salt', 'gold', 'soap', 'roof', 'limb', 'clay', 'lamb', 'mild', 'bald', 'bold', 'disk', 'deck', 'dusk', 'peak',\n",
    "    'pork', 'fork', 'bark'\n",
    "]\n",
    "\n",
    "#ok I need to fix such that there are no \"-oo-\" words otherwise the shuffle check isn't working\n",
    "# Filtering out words with two 'o' from the original list\n",
    "filtered_words = [word for word in four_letter_words if word.count('o') < 2]\n",
    "\n",
    "# Common four-letter words without duplicates from the original list, excluding words with two 'o's\n",
    "additional_replacement_words = [\n",
    "    'cold', 'warm', 'wild', 'wise', 'tall', 'thin', 'tiny', 'rich', 'poor', 'neat', 'lazy', 'kind', 'huge', 'full', 'firm',\n",
    "    'fair', 'easy', 'dark', 'cute', 'calm', 'busy', 'bold', 'bald', 'awed', 'avid', 'arts', 'arms', 'arch', 'apps', 'ants',\n",
    "    'ankh', 'amps', 'alps', 'ally', 'alas', 'akin', 'aide', 'ages', 'aero', 'adds', 'acid', 'aces'\n",
    "]\n",
    "\n",
    "# We need to add enough words to bring the total back up to 153 (why 153?) \n",
    "number_of_words_needed = 153 - len(filtered_words)\n",
    "replacement_words = []\n",
    "\n",
    "for word in additional_replacement_words:\n",
    "    if word not in four_letter_words and len(replacement_words) < number_of_words_needed:\n",
    "        replacement_words.append(word)\n",
    "\n",
    "# Combine the filtered list with the new unique replacement words\n",
    "final_four_letter_words = filtered_words + replacement_words\n",
    "len(final_four_letter_words), final_four_letter_words\n",
    "\n",
    "#I moved this code chunk and make sure to reduce here to 120\n",
    "shuffled_four_pairs = []\n",
    "for word in final_four_letter_words:\n",
    "    shuffled = list(word)\n",
    "    while True:\n",
    "        random.shuffle(shuffled)\n",
    "        shuffled_word = ''.join(shuffled)\n",
    "        if shuffled_word != word:\n",
    "            break\n",
    "    shuffled_four_pairs.append(f\"{word}, {shuffled_word}\")\n",
    "    #now we need to remove pairs down such that the list to 120\n",
    "shuffled_four_pairs = shuffled_four_pairs[:120]\n",
    "\n",
    "shuffled_four_pairs[:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf7d84",
   "metadata": {},
   "source": [
    "<h3> Concatinate and create the Sets </h3>\n",
    "\n",
    "So now we need to make the stimuli for each of the blocks. For this we want to have 10 strings of each length per block for 3 blocks. \n",
    "\n",
    "And then we make groups of those until all the words are assigned. Finally, we will collect 10-30 participants per group. \n",
    "\n",
    "So in total: 3 blocks of 30 anagrams which will be 10 four letters, 10 five letters, 10 six letters. To make sure we get all the words, we will need 4 groups to have the 120 words for each string length represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e3f61ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making a distributing function to spread the words across the four groups\n",
    "def distribute_words(word_list, num_groups=4):\n",
    "    # Calculate the number of words per group\n",
    "    group_size = len(word_list) // num_groups\n",
    "    \n",
    "    # Create groups\n",
    "    groups = [[] for _ in range(num_groups)]\n",
    "    \n",
    "    # Distribute words to each group\n",
    "    for index, word in enumerate(word_list):\n",
    "        group_index = index // group_size\n",
    "        if group_index < num_groups:  # This check prevents index out of range if not perfectly divisible\n",
    "            groups[group_index].append(word)\n",
    "    \n",
    "    return groups\n",
    "\n",
    "\n",
    "# Distribute words to the groups\n",
    "grouped_four_letter_words = distribute_words(shuffled_four_pairs)\n",
    "grouped_five_letter_words = distribute_words(shuffled_pairs_five)\n",
    "grouped_six_letter_words = distribute_words(shuffled_six_pairs)\n",
    "\n",
    "# Save each group to a separate CSV file-- we do this so we can review the words in each group and how they are distributed. \n",
    "\n",
    "import csv\n",
    "for i in range(4):\n",
    "    filename = f'group_{i+1}_word_pairs.csv'\n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Type', 'Word Pairs'])\n",
    "        writer.writerows([['Four-Letter', word] for word in grouped_four_letter_words[i]])\n",
    "        writer.writerows([['Five-Letter', word] for word in grouped_five_letter_words[i]])\n",
    "        writer.writerows([['Six-Letter', word] for word in grouped_six_letter_words[i]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7555b9ca",
   "metadata": {},
   "source": [
    "Okay now we need to create the stimuli file which should be in a .js that will look like: \n",
    "\n",
    "let trial_objects = [\n",
    "    {\n",
    "        \"id\": \"001\",\n",
    "        \"type\": \"Four-Letter\",\n",
    "        \"anagram\": \"atth\",\n",
    "        \"correct\": \"that\",\n",
    "        \"set\": \"A\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7bba816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "csv_directory = \"./group_1_word_pairs.csv\", \"./group_2_word_pairs.csv\", \"./group_3_word_pairs.csv\", \"./group_4_word_pairs.csv\"\n",
    "\n",
    "# Function to convert CSV content to JS format\n",
    "def csv_to_js_format(csv_content, set_name):\n",
    "    js_entries = []\n",
    "    for idx, row in enumerate(csv_content):\n",
    "        word_type, word_pair = row\n",
    "        if idx == 0:  # Skip header row\n",
    "            continue\n",
    "        original, anagram = word_pair.split(\", \")\n",
    "        js_entry = {\n",
    "            \"id\": f\"{str(idx).zfill(3)}\",\n",
    "            \"type\": word_type,\n",
    "            \"anagram\": anagram,\n",
    "            \"correct\": original,\n",
    "            \"set\": set_name\n",
    "        }\n",
    "        js_entries.append(js_entry)\n",
    "    return js_entries\n",
    "\n",
    "\n",
    "# Placeholder for all JS entries\n",
    "all_entries = []\n",
    "\n",
    "# Loop over each CSV file and set name\n",
    "for i in range(4):\n",
    "    filename = f'group_{i+1}_word_pairs.csv'\n",
    "    set_name = f\"Set{chr(65 + i)}\"  # 'SetA', 'SetB', 'SetC', 'SetD'\n",
    "    \n",
    "    with open(filename, newline='') as csvfile:\n",
    "     csvreader = csv.reader(csvfile)\n",
    "     js_entries = csv_to_js_format(csvreader, set_name)  # Pass csvreader and set_name\n",
    "     all_entries.extend(js_entries)\n",
    "\n",
    "\n",
    "\n",
    "## Writing the JS file\n",
    "# Save all entries into the JS file\n",
    "stimuli_js_content = \"let trial_objects = \" + str(all_entries) + \";\"\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(stimuli_js_content)\n",
    "\n",
    "#modfiying the stimuli.js file to be in the preferred format (mostly for readability)\n",
    "# Here we format the JSON with specific spacing and bracketing style\n",
    "stimuli_js_content = \"let trial_objects = [\\n\"\n",
    "for entry in all_entries:\n",
    "    stimuli_js_content += \"    \" + json.dumps(entry, indent=4) + \",\\n\"\n",
    "stimuli_js_content = stimuli_js_content.rstrip(\",\\n\") + \"\\n];\"\n",
    "\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(stimuli_js_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772a2c41",
   "metadata": {},
   "source": [
    "Alright we've now got a set but we are gonna take each set and then shuffle into 4 coded runs.\n",
    "Essentially, by trying to randomize the order on the fly, we can introduce a bunch of bugs into the timeline variables. \n",
    "By hardcoding 4 unique run orders for each set of words, we are addressing order effects without risking more bugs. \n",
    "\n",
    "So below we take the stimuli file, filter by set, add a new parameter to the stimuli file which is its run order assignment which is numbered 1-4. \n",
    "Now ALL of the words in set A will be shuffled into 4 unique orders and assigned to A1, A2, A3, A4. All the A words are the same but their order is now randomized.\n",
    "The this will be saved into the stimuli file as \"SetRun\":\"A1\" etc etc.\n",
    "\n",
    "Now this will make the Stimuli.js file increase substancially (by four) but will keep our code clean and modular. Also fewer headaches since the other option is adjusting the JS utility file that makes the variable order and we really don't wanna do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0afebca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID Occurrences:\n",
      "id 032: 16 times\n",
      "id 085: 16 times\n",
      "id 054: 16 times\n",
      "id 030: 16 times\n",
      "id 044: 16 times\n",
      "id 080: 16 times\n",
      "id 087: 16 times\n",
      "id 028: 16 times\n",
      "id 057: 16 times\n",
      "id 056: 16 times\n",
      "id 011: 16 times\n",
      "id 071: 16 times\n",
      "id 020: 16 times\n",
      "id 048: 16 times\n",
      "id 059: 16 times\n",
      "id 082: 16 times\n",
      "id 069: 16 times\n",
      "id 043: 16 times\n",
      "id 017: 16 times\n",
      "id 047: 16 times\n",
      "id 066: 16 times\n",
      "id 076: 16 times\n",
      "id 014: 16 times\n",
      "id 035: 16 times\n",
      "id 084: 16 times\n",
      "id 090: 16 times\n",
      "id 062: 16 times\n",
      "id 061: 16 times\n",
      "id 021: 16 times\n",
      "id 089: 16 times\n",
      "id 060: 16 times\n",
      "id 064: 16 times\n",
      "id 038: 16 times\n",
      "id 042: 16 times\n",
      "id 026: 16 times\n",
      "id 005: 16 times\n",
      "id 049: 16 times\n",
      "id 037: 16 times\n",
      "id 077: 16 times\n",
      "id 010: 16 times\n",
      "id 016: 16 times\n",
      "id 067: 16 times\n",
      "id 019: 16 times\n",
      "id 055: 16 times\n",
      "id 002: 16 times\n",
      "id 007: 16 times\n",
      "id 046: 16 times\n",
      "id 052: 16 times\n",
      "id 023: 16 times\n",
      "id 003: 16 times\n",
      "id 058: 16 times\n",
      "id 025: 16 times\n",
      "id 008: 16 times\n",
      "id 078: 16 times\n",
      "id 083: 16 times\n",
      "id 009: 16 times\n",
      "id 053: 16 times\n",
      "id 034: 16 times\n",
      "id 012: 16 times\n",
      "id 075: 16 times\n",
      "id 033: 16 times\n",
      "id 074: 16 times\n",
      "id 006: 16 times\n",
      "id 004: 16 times\n",
      "id 070: 16 times\n",
      "id 013: 16 times\n",
      "id 051: 16 times\n",
      "id 039: 16 times\n",
      "id 050: 16 times\n",
      "id 091: 16 times\n",
      "id 031: 16 times\n",
      "id 040: 16 times\n",
      "id 036: 16 times\n",
      "id 088: 16 times\n",
      "id 065: 16 times\n",
      "id 081: 16 times\n",
      "id 063: 16 times\n",
      "id 073: 16 times\n",
      "id 079: 16 times\n",
      "id 072: 16 times\n",
      "id 068: 16 times\n",
      "id 015: 16 times\n",
      "id 027: 16 times\n",
      "id 024: 16 times\n",
      "id 018: 16 times\n",
      "id 086: 16 times\n",
      "id 041: 16 times\n",
      "id 022: 16 times\n",
      "id 045: 16 times\n",
      "id 029: 16 times\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "# Function to convert CSV content to JS format\n",
    "def csv_to_js_format(csv_content, set_name):\n",
    "    js_entries = []\n",
    "    for idx, row in enumerate(csv_content):\n",
    "        if idx == 0:  # Skip header row\n",
    "            continue\n",
    "        word_type, word_pair = row\n",
    "        original, anagram = word_pair.split(\", \")\n",
    "        js_entry = {\n",
    "            \"id\": f\"{str(idx + 1).zfill(3)}\",  # Assign unique IDs starting from 001\n",
    "            \"type\": word_type,\n",
    "            \"anagram\": anagram,\n",
    "            \"correct\": original,\n",
    "            \"set\": set_name\n",
    "        }\n",
    "        js_entries.append(js_entry)\n",
    "    return js_entries\n",
    "\n",
    "# Function to shuffle a set of words and include set run order\n",
    "def set_shuffle(word_list, set_name, run_number):\n",
    "    shuffled_list = word_list[:]  # Create a copy of the word_list to shuffle\n",
    "    random.shuffle(shuffled_list)  # Shuffle the list of words\n",
    "    set_run = f\"{set_name}{run_number}\"\n",
    "    return [{\n",
    "        \"id\": word[\"id\"],\n",
    "        \"type\": word[\"type\"],\n",
    "        \"anagram\": word[\"anagram\"],\n",
    "        \"correct\": word[\"correct\"],\n",
    "        \"set\": word[\"set\"],\n",
    "        \"setRun\": set_run\n",
    "    } for word in shuffled_list]\n",
    "\n",
    "# Placeholder for all JS entries\n",
    "all_entries = []\n",
    "\n",
    "# Loop over each CSV file and set name\n",
    "for i in range(4):\n",
    "    filename = f'group_{i+1}_word_pairs.csv'\n",
    "    set_name = f\"Set{chr(65 + i)}\"  # 'SetA', 'SetB', 'SetC', 'SetD'\n",
    "    \n",
    "    with open(filename, newline='') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        csv_content = list(csvreader)  # Convert csvreader to a list\n",
    "        js_entries = csv_to_js_format(csv_content, set_name)  # Pass csv_content and set_name\n",
    "        for run_number in range(1, 5):  # Create 4 runs for each set\n",
    "            shuffled_set = set_shuffle(js_entries, set_name, run_number)\n",
    "            all_entries.extend(shuffled_set)\n",
    "\n",
    "# Save all entries into the JS file\n",
    "stimuli_js_content = \"let trial_objects = \" + json.dumps(all_entries, indent=4) + \";\"\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(stimuli_js_content)\n",
    "\n",
    "# Count occurrences of each id... okay well this isn't right but also not really important, so I'll leave it as is. What's happening is that the id is being counted but the assignment is made during a loop and so for each id there is 4 unique string pairs. for example 032 is about, water, birth, and trust.\n",
    "id_counter = Counter(entry[\"id\"] for entry in all_entries)\n",
    "print(\"ID Occurrences:\")\n",
    "for id_, count in id_counter.items():\n",
    "    print(f\"id {id_}: {count} times\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caac0b83",
   "metadata": {},
   "source": [
    "Okay I'm just looking for a sanity check and gonna run a loop over the stimuli js file to correct the  ID and then see if that does it. I think this notebook should be a code review for lab someday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0785e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New ID mapping for unique anagrams:\n",
      "Anagram: otbau, New ID: 001\n",
      "Anagram: yeowll, New ID: 002\n",
      "Anagram: lilts, New ID: 003\n",
      "Anagram: eorv, New ID: 004\n",
      "Anagram: eanrl, New ID: 005\n",
      "Anagram: ilervs, New ID: 006\n",
      "Anagram: adorab, New ID: 007\n",
      "Anagram: saol, New ID: 008\n",
      "Anagram: trehe, New ID: 009\n",
      "Anagram: iethr, New ID: 010\n",
      "Anagram: enhw, New ID: 011\n",
      "Anagram: gjlenu, New ID: 012\n",
      "Anagram: eryv, New ID: 013\n",
      "Anagram: latnp, New ID: 014\n",
      "Anagram: hgtin, New ID: 015\n",
      "Anagram: ufselu, New ID: 016\n",
      "Anagram: hhaelt, New ID: 017\n",
      "Anagram: grael, New ID: 018\n",
      "Anagram: mite, New ID: 019\n",
      "Anagram: leacp, New ID: 020\n",
      "Anagram: ffctee, New ID: 021\n",
      "Anagram: bjecto, New ID: 022\n",
      "Anagram: osme, New ID: 023\n",
      "Anagram: ginaa, New ID: 024\n",
      "Anagram: ewtlha, New ID: 025\n",
      "Anagram: tderes, New ID: 026\n",
      "Anagram: ncotai, New ID: 027\n",
      "Anagram: eehrt, New ID: 028\n",
      "Anagram: konw, New ID: 029\n",
      "Anagram: eclcri, New ID: 030\n",
      "Anagram: ktihn, New ID: 031\n",
      "Anagram: nhgace, New ID: 032\n",
      "Anagram: evyre, New ID: 033\n",
      "Anagram: husoe, New ID: 034\n",
      "Anagram: vgie, New ID: 035\n",
      "Anagram: mfor, New ID: 036\n",
      "Anagram: ponti, New ID: 037\n",
      "Anagram: dolcu, New ID: 038\n",
      "Anagram: peleop, New ID: 039\n",
      "Anagram: hwat, New ID: 040\n",
      "Anagram: itno, New ID: 041\n",
      "Anagram: gltihf, New ID: 042\n",
      "Anagram: omer, New ID: 043\n",
      "Anagram: ysdtu, New ID: 044\n",
      "Anagram: ttha, New ID: 045\n",
      "Anagram: heav, New ID: 046\n",
      "Anagram: hotre, New ID: 047\n",
      "Anagram: onusd, New ID: 048\n",
      "Anagram: antw, New ID: 049\n",
      "Anagram: hwti, New ID: 050\n",
      "Anagram: sheet, New ID: 051\n",
      "Anagram: akbc, New ID: 052\n",
      "Anagram: tyeh, New ID: 053\n",
      "Anagram: azutrq, New ID: 054\n",
      "Anagram: uavcmu, New ID: 055\n",
      "Anagram: iwll, New ID: 056\n",
      "Anagram: llpse, New ID: 057\n",
      "Anagram: eatrf, New ID: 058\n",
      "Anagram: enht, New ID: 059\n",
      "Anagram: tnurea, New ID: 060\n",
      "Anagram: beoav, New ID: 061\n",
      "Anagram: emdtho, New ID: 062\n",
      "Anagram: oruy, New ID: 063\n",
      "Anagram: itsh, New ID: 064\n",
      "Anagram: iansld, New ID: 065\n",
      "Anagram: hemt, New ID: 066\n",
      "Anagram: lmsal, New ID: 067\n",
      "Anagram: sitrf, New ID: 068\n",
      "Anagram: gthir, New ID: 069\n",
      "Anagram: ygeren, New ID: 070\n",
      "Anagram: uhcm, New ID: 071\n",
      "Anagram: ofndu, New ID: 072\n",
      "Anagram: oblwe, New ID: 073\n",
      "Anagram: rebeez, New ID: 074\n",
      "Anagram: daaemg, New ID: 075\n",
      "Anagram: erlvat, New ID: 076\n",
      "Anagram: uebtay, New ID: 077\n",
      "Anagram: dlqiui, New ID: 078\n",
      "Anagram: oredcr, New ID: 079\n",
      "Anagram: keelrn, New ID: 080\n",
      "Anagram: rwohgt, New ID: 081\n",
      "Anagram: tkea, New ID: 082\n",
      "Anagram: stmo, New ID: 083\n",
      "Anagram: txet, New ID: 084\n",
      "Anagram: lkie, New ID: 085\n",
      "Anagram: przhye, New ID: 086\n",
      "Anagram: gtrae, New ID: 087\n",
      "Anagram: tahn, New ID: 088\n",
      "Anagram: eenrv, New ID: 089\n",
      "Anagram: wdno, New ID: 090\n",
      "Anagram: ndif, New ID: 091\n",
      "Anagram: ripeem, New ID: 092\n",
      "Anagram: faodfr, New ID: 093\n",
      "Anagram: rrmoir, New ID: 094\n",
      "Anagram: yyrale, New ID: 095\n",
      "Anagram: twire, New ID: 096\n",
      "Anagram: oyugn, New ID: 097\n",
      "Anagram: htigl, New ID: 098\n",
      "Anagram: wteall, New ID: 099\n",
      "Anagram: brorha, New ID: 100\n",
      "Anagram: ecagrh, New ID: 101\n",
      "Anagram: eevn, New ID: 102\n",
      "Anagram: ofcife, New ID: 103\n",
      "Anagram: higts, New ID: 104\n",
      "Anagram: tfhrea, New ID: 105\n",
      "Anagram: ecfa, New ID: 106\n",
      "Anagram: leiv, New ID: 107\n",
      "Anagram: mihgt, New ID: 108\n",
      "Anagram: niel, New ID: 109\n",
      "Anagram: lalc, New ID: 110\n",
      "Anagram: elvo, New ID: 111\n",
      "Anagram: ielhw, New ID: 112\n",
      "Anagram: phel, New ID: 113\n",
      "Anagram: odlwu, New ID: 114\n",
      "Anagram: atrts, New ID: 115\n",
      "Anagram: genrad, New ID: 116\n",
      "Anagram: ehwti, New ID: 117\n",
      "Anagram: eflt, New ID: 118\n",
      "Anagram: wihhc, New ID: 119\n",
      "Anagram: ruesp, New ID: 120\n",
      "Anagram: vome, New ID: 121\n",
      "Anagram: rnut, New ID: 122\n",
      "Anagram: vnisoi, New ID: 123\n",
      "Anagram: emse, New ID: 124\n",
      "Anagram: weset, New ID: 125\n",
      "Anagram: awya, New ID: 126\n",
      "Anagram: atkl, New ID: 127\n",
      "Anagram: ewrhe, New ID: 128\n",
      "Anagram: jniruy, New ID: 129\n",
      "Anagram: kuicq, New ID: 130\n",
      "Anagram: priist, New ID: 131\n",
      "Anagram: agpe, New ID: 132\n",
      "Anagram: paly, New ID: 133\n",
      "Anagram: cekjta, New ID: 134\n",
      "Anagram: psto, New ID: 135\n",
      "Anagram: anlxie, New ID: 136\n",
      "Anagram: eelf, New ID: 137\n",
      "Anagram: lneeed, New ID: 138\n",
      "Anagram: mrtpio, New ID: 139\n",
      "Anagram: dptuae, New ID: 140\n",
      "Anagram: lissof, New ID: 141\n",
      "Anagram: tpra, New ID: 142\n",
      "Anagram: dgrnae, New ID: 143\n",
      "Anagram: gener, New ID: 144\n",
      "Anagram: aecrl, New ID: 145\n",
      "Anagram: klelri, New ID: 146\n",
      "Anagram: yotsr, New ID: 147\n",
      "Anagram: ekpe, New ID: 148\n",
      "Anagram: evuqri, New ID: 149\n",
      "Anagram: yict, New ID: 150\n",
      "Anagram: nowbr, New ID: 151\n",
      "Anagram: ocdiaz, New ID: 152\n",
      "Anagram: iuteq, New ID: 153\n",
      "Anagram: rdah, New ID: 154\n",
      "Anagram: uqiet, New ID: 155\n",
      "Anagram: eocm, New ID: 156\n",
      "Anagram: nderu, New ID: 157\n",
      "Anagram: meltep, New ID: 158\n",
      "Anagram: srue, New ID: 159\n",
      "Anagram: uitrag, New ID: 160\n",
      "Anagram: inlpec, New ID: 161\n",
      "Anagram: uspl, New ID: 162\n",
      "Anagram: hoest, New ID: 163\n",
      "Anagram: atsy, New ID: 164\n",
      "Anagram: habcrn, New ID: 165\n",
      "Anagram: altpop, New ID: 166\n",
      "Anagram: erad, New ID: 167\n",
      "Anagram: hgtir, New ID: 168\n",
      "Anagram: sdnat, New ID: 169\n",
      "Anagram: dnee, New ID: 170\n",
      "Anagram: tnigh, New ID: 171\n",
      "Anagram: hmerma, New ID: 172\n",
      "Anagram: aklbc, New ID: 173\n",
      "Anagram: aretw, New ID: 174\n",
      "Anagram: tsaet, New ID: 175\n",
      "Anagram: okrw, New ID: 176\n",
      "Anagram: enxt, New ID: 177\n",
      "Anagram: wrdol, New ID: 178\n",
      "Anagram: rngay, New ID: 179\n",
      "Anagram: esonra, New ID: 180\n",
      "Anagram: amyn, New ID: 181\n",
      "Anagram: ehar, New ID: 182\n",
      "Anagram: leovit, New ID: 183\n",
      "Anagram: htire, New ID: 184\n",
      "Anagram: cabneo, New ID: 185\n",
      "Anagram: unaitq, New ID: 186\n",
      "Anagram: feil, New ID: 187\n",
      "Anagram: lbea, New ID: 188\n",
      "Anagram: ceeap, New ID: 189\n",
      "Anagram: ensev, New ID: 190\n",
      "Anagram: tintek, New ID: 191\n",
      "Anagram: btse, New ID: 192\n",
      "Anagram: nhakt, New ID: 193\n",
      "Anagram: magtne, New ID: 194\n",
      "Anagram: ehrat, New ID: 195\n",
      "Anagram: tsco, New ID: 196\n",
      "Anagram: srhfe, New ID: 197\n",
      "Anagram: eedgln, New ID: 198\n",
      "Anagram: enses, New ID: 199\n",
      "Anagram: rthib, New ID: 200\n",
      "Anagram: eotn, New ID: 201\n",
      "Anagram: lsat, New ID: 202\n",
      "Anagram: ameg, New ID: 203\n",
      "Anagram: datoy, New ID: 204\n",
      "Anagram: oeph, New ID: 205\n",
      "Anagram: rtidh, New ID: 206\n",
      "Anagram: ujrion, New ID: 207\n",
      "Anagram: nithk, New ID: 208\n",
      "Anagram: emta, New ID: 209\n",
      "Anagram: nadelc, New ID: 210\n",
      "Anagram: inecs, New ID: 211\n",
      "Anagram: oattl, New ID: 212\n",
      "Anagram: cnevio, New ID: 213\n",
      "Anagram: lrati, New ID: 214\n",
      "Anagram: htorn, New ID: 215\n",
      "Anagram: oxpetr, New ID: 216\n",
      "Anagram: lelt, New ID: 217\n",
      "Anagram: iebozm, New ID: 218\n",
      "Anagram: rerntu, New ID: 219\n",
      "Anagram: ecipe, New ID: 220\n",
      "Anagram: rteta, New ID: 221\n",
      "Anagram: ypatr, New ID: 222\n",
      "Anagram: llwyeo, New ID: 223\n",
      "Anagram: eahtr, New ID: 224\n",
      "Anagram: odrpu, New ID: 225\n",
      "Anagram: shxti, New ID: 226\n",
      "Anagram: month, New ID: 227\n",
      "Anagram: ghnti, New ID: 228\n",
      "Anagram: eljgug, New ID: 229\n",
      "Anagram: sace, New ID: 230\n",
      "Anagram: onreag, New ID: 231\n",
      "Anagram: nwrche, New ID: 232\n",
      "Anagram: retmka, New ID: 233\n",
      "Anagram: reac, New ID: 234\n",
      "Anagram: dheta, New ID: 235\n",
      "Anagram: enurgt, New ID: 236\n",
      "Anagram: alshl, New ID: 237\n",
      "Anagram: ddceea, New ID: 238\n",
      "Anagram: ifon, New ID: 239\n",
      "Anagram: mhoe, New ID: 240\n",
      "Anagram: lmsiep, New ID: 241\n",
      "Anagram: orad, New ID: 242\n",
      "Anagram: eeagul, New ID: 243\n",
      "Anagram: roezfn, New ID: 244\n",
      "Anagram: estt, New ID: 245\n",
      "Anagram: gspsoi, New ID: 246\n",
      "Anagram: apictm, New ID: 247\n",
      "Anagram: othrs, New ID: 248\n",
      "Anagram: matoun, New ID: 249\n",
      "Anagram: eurt, New ID: 250\n",
      "Anagram: patten, New ID: 251\n",
      "Anagram: arnrwo, New ID: 252\n",
      "Anagram: mtos, New ID: 253\n",
      "Anagram: ontesh, New ID: 254\n",
      "Anagram: utcoh, New ID: 255\n",
      "Anagram: npal, New ID: 256\n",
      "Anagram: opcy, New ID: 257\n",
      "Anagram: erhte, New ID: 258\n",
      "Anagram: ydikne, New ID: 259\n",
      "Anagram: iwve, New ID: 260\n",
      "Anagram: owsh, New ID: 261\n",
      "Anagram: ieda, New ID: 262\n",
      "Anagram: ekew, New ID: 263\n",
      "Anagram: yodb, New ID: 264\n",
      "Anagram: stei, New ID: 265\n",
      "Anagram: dahn, New ID: 266\n",
      "Anagram: itaw, New ID: 267\n",
      "Anagram: stiesu, New ID: 268\n",
      "Anagram: nilfa, New ID: 269\n",
      "Anagram: arint, New ID: 270\n",
      "Anagram: yttnko, New ID: 271\n",
      "Anagram: lndsai, New ID: 272\n",
      "Anagram: twhac, New ID: 273\n",
      "Anagram: tsli, New ID: 274\n",
      "Anagram: hself, New ID: 275\n",
      "Anagram: yngou, New ID: 276\n",
      "Anagram: msis, New ID: 277\n",
      "Anagram: ellyav, New ID: 278\n",
      "Anagram: wtroh, New ID: 279\n",
      "Anagram: dpaeut, New ID: 280\n",
      "Anagram: enowrd, New ID: 281\n",
      "Anagram: sure, New ID: 282\n",
      "Anagram: shif, New ID: 283\n",
      "Anagram: rcad, New ID: 284\n",
      "Anagram: wnmeo, New ID: 285\n",
      "Anagram: piarll, New ID: 286\n",
      "Anagram: edal, New ID: 287\n",
      "Anagram: aedgma, New ID: 288\n",
      "Anagram: rfom, New ID: 289\n",
      "Anagram: qruuom, New ID: 290\n",
      "Anagram: rcanhb, New ID: 291\n",
      "Anagram: oevci, New ID: 292\n",
      "Anagram: olkcc, New ID: 293\n",
      "Anagram: elas, New ID: 294\n",
      "Anagram: lenion, New ID: 295\n",
      "Anagram: aybb, New ID: 296\n",
      "Anagram: ssma, New ID: 297\n",
      "Anagram: nrlaom, New ID: 298\n",
      "Anagram: waets, New ID: 299\n",
      "Anagram: gowrn, New ID: 300\n",
      "Anagram: otinpo, New ID: 301\n",
      "Anagram: oveid, New ID: 302\n",
      "Anagram: labl, New ID: 303\n",
      "Anagram: huotys, New ID: 304\n",
      "Anagram: pisl, New ID: 305\n",
      "Anagram: lbcu, New ID: 306\n",
      "Anagram: auvel, New ID: 307\n",
      "Anagram: ogetun, New ID: 308\n",
      "Anagram: asdn, New ID: 309\n",
      "Anagram: iitsv, New ID: 310\n",
      "Anagram: erwso, New ID: 311\n",
      "Anagram: rauesq, New ID: 312\n",
      "Anagram: rtewa, New ID: 313\n",
      "Anagram: truts, New ID: 314\n",
      "Anagram: iwrte, New ID: 315\n",
      "Anagram: etps, New ID: 316\n",
      "Anagram: uausl, New ID: 317\n",
      "Anagram: kins, New ID: 318\n",
      "Anagram: tcwie, New ID: 319\n",
      "Anagram: rtuht, New ID: 320\n",
      "Anagram: tuyho, New ID: 321\n",
      "Anagram: ehicoc, New ID: 322\n",
      "Anagram: ituln, New ID: 323\n",
      "Anagram: zigagz, New ID: 324\n",
      "Anagram: ilmk, New ID: 325\n",
      "Anagram: srki, New ID: 326\n",
      "Anagram: hiwel, New ID: 327\n",
      "Anagram: saps, New ID: 328\n",
      "Anagram: oact, New ID: 329\n",
      "Anagram: ecepir, New ID: 330\n",
      "Anagram: gtreat, New ID: 331\n",
      "Anagram: vleo, New ID: 332\n",
      "Anagram: aebs, New ID: 333\n",
      "Anagram: memidu, New ID: 334\n",
      "Anagram: eltlit, New ID: 335\n",
      "Anagram: eert, New ID: 336\n",
      "Anagram: lpaep, New ID: 337\n",
      "Anagram: eowhl, New ID: 338\n",
      "Anagram: foluyj, New ID: 339\n",
      "Anagram: laolgb, New ID: 340\n",
      "Anagram: azjz, New ID: 341\n",
      "Anagram: buel, New ID: 342\n",
      "Anagram: rcea, New ID: 343\n",
      "Anagram: rwoyr, New ID: 344\n",
      "Anagram: oecxti, New ID: 345\n",
      "Anagram: admrei, New ID: 346\n",
      "Anagram: omnaw, New ID: 347\n",
      "Anagram: eunrd, New ID: 348\n",
      "Anagram: roeich, New ID: 349\n",
      "Anagram: lilb, New ID: 350\n",
      "Anagram: rhaic, New ID: 351\n",
      "Anagram: bnda, New ID: 352\n",
      "Anagram: pseong, New ID: 353\n",
      "Anagram: rcinpe, New ID: 354\n",
      "Anagram: ymafli, New ID: 355\n",
      "Anagram: wieht, New ID: 356\n",
      "Anagram: ryma, New ID: 357\n",
      "Anagram: lodwr, New ID: 358\n",
      "Anagram: acpm, New ID: 359\n",
      "Anagram: iqnuue, New ID: 360\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the stimuli file\n",
    "with open(\"stimuli.js\", \"r\") as file:\n",
    "    stimuli_js_content = file.read()\n",
    "\n",
    "# Extract the JSON data from the stimuli file\n",
    "json_data = json.loads(stimuli_js_content[len(\"let trial_objects = \"):-1])\n",
    "\n",
    "# Create a mapping for unique anagrams to new IDs\n",
    "unique_anagram_to_id = {}\n",
    "id_counter = 1\n",
    "\n",
    "for entry in json_data:\n",
    "    anagram = entry[\"anagram\"]\n",
    "    if anagram not in unique_anagram_to_id:\n",
    "        unique_anagram_to_id[anagram] = f\"{id_counter:03d}\"\n",
    "        id_counter += 1\n",
    "\n",
    "# Reassign IDs in the JSON data\n",
    "for entry in json_data:\n",
    "    entry[\"id\"] = unique_anagram_to_id[entry[\"anagram\"]]\n",
    "\n",
    "# Save the updated entries into the JS file\n",
    "updated_stimuli_js_content = \"let trial_objects = \" + json.dumps(json_data, indent=4) + \";\"\n",
    "with open(\"updated_stimuli.js\", \"w\") as file:\n",
    "    file.write(updated_stimuli_js_content)\n",
    "\n",
    "# Print the new ID mapping for verification\n",
    "print(\"New ID mapping for unique anagrams:\")\n",
    "for anagram, new_id in unique_anagram_to_id.items():\n",
    "    print(f\"Anagram: {anagram}, New ID: {new_id}\")\n",
    "\n",
    "# This worked okay lets save to the stimuli.js file\n",
    "with open(\"stimuli.js\", \"w\") as file:\n",
    "    file.write(updated_stimuli_js_content)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python (miniconda3)",
   "language": "python",
   "name": "miniconda3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
